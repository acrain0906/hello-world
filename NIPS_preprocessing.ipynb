{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, DistanceMetric\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "\n",
    "%config InlineBackend.figure_format='svg'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('nips-2015-papers/Papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers = df.to_dict('list')['PaperText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grand_paper = ''\n",
    "for paper in papers:\n",
    "    grand_paper += paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = grand_paper.split('\\n')\n",
    "new_paper = ''\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    if len(lines[i].split(' ')) < 5:\n",
    "        del lines[i]\n",
    "    else:\n",
    "        new_paper += lines[i] + ' '\n",
    "        i += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers2 = new_paper[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers2 = papers2.replace('!','.')\n",
    "papers2 = papers2.replace('?','.')\n",
    "sentences = papers2.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_sent = []\n",
    "count = 0\n",
    "for i in range(len(sentences)):\n",
    "    sent = sentences[i].replace(' ', '')\n",
    "    if len(sent) > 0 and sent[0].isupper():\n",
    "        new_sent.append(sentences[i])\n",
    "    else:\n",
    "        new_sent[-1] += '. ' + sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(new_sent):\n",
    "    new_sent[i] = new_sent[i].replace('“', '').replace('”', '').replace('’', '')\n",
    "    if not any(char.isdigit() for char in new_sent[i]) and len(new_sent[i].split(' ')) > 5 and all(ord(c) < 128 for c in new_sent[i]):\n",
    "        new_sent[i] = new_sent[i].translate(str.maketrans('','',string.punctuation))\n",
    "        i+=1\n",
    "    else:\n",
    "        del new_sent[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data',\n",
       " 'Crowdsourcing is cheap and fast but suffers from the problem of lowquality data',\n",
       " 'To address this fundamental challenge in crowdsourcing we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest',\n",
       " 'We show that surprisingly under a mild and natural nofreelunch requirement this mechanism is the one and only incentivecompatible payment mechanism possible',\n",
       " 'We also show that among all possible incentivecompatible mechanisms that may or may not satisfy nofreelunch our mechanism makes the smallest possible payment to spammers',\n",
       " 'Interestingly this unique mechanism takes a multiplicative form',\n",
       " 'The simplicity of the mechanism is an added benefit',\n",
       " 'In preliminary experiments involving over several hundred workers we observe a significant reduction in the error rates under our unique mechanism for the same Complex machine learning tools such as deep learning are gaining increasing popularity and are being applied to a wide variety of problems',\n",
       " 'These large labeling tasks are being performed by coordinating crowds of semiskilled workers through the Internet',\n",
       " 'Crowdsourcing as a means of collecting labeled training data has now become indispensable to the Most workers in crowdsourcing are not experts',\n",
       " 'However when the inputs to these algorithms are erroneous it is difficult to guarantee that the processed labels will be reliable enough for subsequent use by machine learning or other applications',\n",
       " 'In order to avoid garbage in garbage out we take a complementary approach to this problem cleaning the data at the time of We consider crowdsourcing settings where the workers are paid for their services such as in the popular crowdsourcing platforms of Amazon Mechanical Turk and others',\n",
       " 'These commercial platforms have gained substantial popularity due to their support for a diverse range of tasks for machine learning labeling varying from image annotation and text recognition to speech captioning and machine translation',\n",
       " 'We consider problems that are objective in nature that is have a definite answer',\n",
       " 'Is this the Golden Gate Bridge',\n",
       " 'As a result workers attempt to answer questions that they are not sure of thereby increasing the error rate of the labels',\n",
       " 'Our goal is to develop payment mechanisms to encourage the worker to select this option when she is unsure',\n",
       " 'We will term any payment mechanism that incentivizes the worker to do so as incentive compatible',\n",
       " 'In addition to incentive compatibility preventing spammers is another desirable requirement from incentive mechanisms in crowdsourcing',\n",
       " 'It is thus of interest to deter spammers by paying them as low as possible',\n",
       " 'An intuitive objective to this end is to ensure a zero expenditure on spammers who answer randomly',\n",
       " 'In this paper however we impose a strictly and significantly weaker condition and then show that there is one and only one incentivecompatible mechanism that can satisfy this weak condition',\n",
       " 'Our requirement referred to as the nofreelunch axiom says that if all the questions attempted by the worker are answered incorrectly then the payment must be zero',\n",
       " 'We propose a payment mechanism for the aforementioned setting incentive compatibility plus nofreelunch and show that surprisingly this is the only possible mechanism',\n",
       " 'We also show that additionally our mechanism makes the smallest possible payment to spammers among all possible incentive compatible mechanisms that may or may not satisfy the nofreelunch axiom',\n",
       " 'Our payment mechanism takes a multiplicative form the evaluation of the workers response to each question is a certain score and the final payment is a product of these scores',\n",
       " 'This mechanism has additional appealing features in that it is simple to compute and is also simple to explain to the workers',\n",
       " 'Our mechanism is applicable to any type of objective questions including multiple choice annotation In order to test whether our mechanism is practical and to assess the quality of the final labels obtained we conducted experiments on the Amazon Mechanical Turk crowdsourcing platform',\n",
       " 'In our preliminary experiments that involved over several hundred workers we found that the quality of data improved by twofold under our unique mechanism with the total monetary expenditure being the same or lower as compared to the conventional baseline',\n",
       " 'In the crowdsourcing setting that we consider one or more workers perform a task where a task consists of multiple questions',\n",
       " 'The questions are objective by which we mean each question has precisely one correct answer',\n",
       " 'For any possible answer to any question we define the workers confidence about an answer as the probability according to her belief of this answer being correct',\n",
       " 'In other words one can assume that the worker has in her mind a probability distribution over all possible answers to a question and the confidence for an answer is the probability of that answer being correct',\n",
       " 'As a shorthand we also define the confidence about a question as the confidence for the answer that the worker is most \\x0cconfident about for that question',\n",
       " 'We assume that the workers confidences for different questions are independent',\n",
       " 'The threshold T may be chosen based on various factors of the problem at hand for example on the downstream machine learning algorithms using the crowdsourced data or the knowledge of the statistics of worker abilities etc',\n",
       " 'In this paper we assume that the threshold T is given to us',\n",
       " 'Let N denote the total number of questions in the task',\n",
       " 'Among these we assume the existence of some gold standard questions that is a set of questions whose answers are known to the requester',\n",
       " 'The G gold standard questions are assumed to be distributed uniformly at random in the pool of N questions of course the worker does not know which G of the N questions form the gold standard',\n",
       " 'The payment to a worker for a task is computed after receiving her responses to all the questions in the task',\n",
       " 'The payment is based on the workers performance on the gold standard questions',\n",
       " 'Since the payment is based on known answers the payments to different workers do not depend on each other thereby allowing us to consider the presence of only one worker without any loss in generality',\n",
       " 'We will employ the following standard notation',\n",
       " 'The notation R denotes the set of all nonnegative real numbers',\n",
       " 'Note that the crowdsourcing platforms of today mandate the payments to be nonnegative',\n",
       " 'We assume that the worker attempts to maximize her overall expected payment',\n",
       " 'The inner summation corresponds to the expectation with respect to the workers beliefs about the correctness of her In the event that the confidence about a question is exactly equal to T  the worker may be equally incentivized to answer or skip \\x0cWe will call any payment function f as an incentivecompatible mechanism if the expected payment of the worker under this payment function is strictly maximized when the worker responds in the Main results Incentivecompatible mechanism and guarantees In this section we present the main results of the paper namely the design of incentivecompatible mechanisms with practically useful properties',\n",
       " 'If all the answers attempted by the worker in the gold standard are wrong then the payment is zero',\n",
       " 'Observe that nofreelunch is an extremely mild requirement',\n",
       " 'In fact it is significantly weaker than imposing a zero payment on workers who answer randomly',\n",
       " 'However if any of these questions are answered incorrectly then the reward will become zero',\n",
       " 'The algorithm makes a zero payment if one or more attempted answers in the gold standard are wrong',\n",
       " 'Note that this property is significantly stronger than the property of nofreelunch which we originally required where we wanted a zero payment only when all attempted answers were wrong',\n",
       " 'In the latter case the worker is incentivized to select the answer which she thinks is most likely to be correct',\n",
       " 'It is easy to see that the mechanism satisfies nofreelunch',\n",
       " 'The proof of incentive compatibility is also not hard We consider any arbitrary worker with arbitrary belief distributions and compute the expected payment for that worker for the case when her choices in the task follow the requirements',\n",
       " 'A natural question that arises is can we design an alternative mechanism satisfying incentive compatibility and nofreelunch that operates somewhere in between',\n",
       " 'In the previous section we showed that our proposed multiplicative mechanism is incentive compatible and satisfies the intuitive requirement of nofreelunch',\n",
       " 'It turns out perhaps surprisingly that this mechanism is unique in this respect',\n",
       " 'To see this recall our earlier discussion on deterring spammers that is incurring a low expenditure on workers who answer randomly',\n",
       " 'The proof relies on the following key lemma that establishes a condition that any incentivecompatible mechanism must necessarily satisfy',\n",
       " 'The lemma applies to any incentivecompatible mechanism and not just to those satisfying nofreelunch',\n",
       " 'The proof of this lemma is provided in Appendix C',\n",
       " 'As discussed earlier crowdsouring tasks especially those with multiple choice questions often encounter spammers who answer randomly without heed to the question being asked',\n",
       " 'For instance under a binarychoice setup a spammer will choose one of the two options uniformly at random for every question',\n",
       " 'A highly desirable objective in crowdsourcing settings is to deter spammers',\n",
       " 'A second desirable metric could be to minimize the expenditure on a worker who simply skips all questions',\n",
       " 'While the aforementioned requirements were deterministic functions of the workers responses one may alternatively wish to impose requirements that depend on the distribution of the workers answering process',\n",
       " 'For instance a third desirable feature would be to minimize the expected payment to a worker who answers all questions uniformly at random',\n",
       " 'We now show that interestingly our unique multiplicative payment mechanism simultaneously satisfies all these requirements',\n",
       " 'The result is stated assuming a multiplechoice setup but extends trivially to nonmultiplechoice settings',\n",
       " 'Consider the popular class of additive mechanisms where the payments to a worker are added across the gold standard questions',\n",
       " 'Importantly the final payment to the worker is the sum of the rewards across the G gold standard questions',\n",
       " 'One can verify that this additive mechanism is incentive compatible',\n",
       " 'One can also see that that as guaranteed by our theory this additive payment mechanism does not Suppose each question involves choosing from two options',\n",
       " 'Let us compute the expenditure that these two mechanisms make under a spamming behavior of choosing the answer randomly to each question',\n",
       " 'The payment to spammers thus reduces exponentially with the number of gold standard questions under our mechanism whereas it does not reduce at all in the Now consider a different means of exploiting the mechanisms where the worker simply skips all questions',\n",
       " 'In this section we present synthetic simulations and realworld experiments to evaluate the effects of our setting and our mechanism on the final label quality',\n",
       " 'We employ synthetic simulations to understand the effects of various kinds of labeling errors in crowdsourcing',\n",
       " 'We consider binarychoice questions in this set of simulations',\n",
       " 'Whenever a worker answers a question her confidence for the correct answer is drawn from a distribution P independent of all else',\n",
       " 'We compare a the setting where workers attempt every question with b the setting where workers skip questions for which their confidence is below a certain threshold T ',\n",
       " 'In either setting we aggregate the labels obtained from the workers for each question via a majority vote on the two classes',\n",
       " 'In particular the gains are quite striking under the hammerspammer model this result is not surprising since the mechanism ideally screens the spammers out and leaves only the Experiments on Amazon Mechanical Turk We conducted preliminary experiments on the Amazon Mechanical Turk commercial crowdsourcing platform mturkcom to evaluate our proposed scheme in realworld scenarios',\n",
       " 'The complete data including the interface presented to the workers in each of the tasks the results obtained from the workers and the ground truth solutions are available on the website of the first author',\n",
       " 'Before delving into details we first note certain caveats relating to such a study of mechanism design on crowdsourcing platforms',\n",
       " 'When a worker encounters a mechanism for only a small amount of time a handful of tasks in typical research experiments and for a small amount of money at most a few dollars in typical crowdsourcing tasks we cannot expect the worker to completely understand the mechanism and act precisely as required',\n",
       " 'For instance we wouldnt expect our experimental results to change significantly even upon moderate modifications in the promised amounts and furthermore we do expect the outcomes to be noisy',\n",
       " 'Incentive compatibility kicks in when the worker encounters a mechanism across a longer term for example when a proposed mechanism is adopted as a standard for a platform or when higher amounts are involved',\n",
       " 'This is when we would expect workers or others eg bloggers or researchers to design strategies that can game the mechanism',\n",
       " 'The theoretical guarantee of incentive compatibility or strict properness then prevents such gaming in the long run',\n",
       " 'We thus regard these experiments as preliminary',\n",
       " 'Our intentions towards this experimental exercise were a to evaluate the potential of our algorithms to work in practice and b to investigate the effect of the proposed algorithms on the net error in the collected labelled data',\n",
       " 'We conducted the five following experiments tasks on Amazon Mechanical Turk a identifying the golden gate bridge from pictures b identifying the breeds of dogs from pictures c identifying heads of countries d identifying continents to which flags belong and e identifying the textures in displayed images',\n",
       " 'Upon completion of the tasks on Amazon Mechanical Turk we aggregated the data in the following manner',\n",
       " 'We see that in most cases our skipbased setting results in a higher quality data and in many of the instances the reduction is twofold or higher',\n",
       " 'All in all in the experiments we observed a substantial reduction in the amount of error in the labelled data while expending the same or lower amounts and receiving no negative comments from the workers',\n",
       " 'These observations suggest that our proposed skipbased setting coupled with our multiplicative payment mechanisms have potential to work in practice the underlying fundamental theory ensures that the system cannot be gamed in the long run',\n",
       " 'Our mechanism offers some additional benefits',\n",
       " 'The pattern of skips of the workers provide a reasonable estimate of the difficulty of each question',\n",
       " 'In practice the questions that are estimated to be more difficult may now be delegated to an expert or to additional nonexpert workers',\n",
       " 'Secondly the theoretical guarantees of our mechanism may allow for better postprocessing of the data incorporating the confidence information and improving the overall accuracy',\n",
       " 'Thirdly the simplicity of our mechanisms may facilitate an easier adoption among the workers',\n",
       " 'In conclusion given the uniqueness and optimality in theory simplicity and good performance observed in practice we envisage our multiplicative payment mechanisms to be of interest to practitioners as well as researchers who employ crowdsourcing',\n",
       " 'Andrew Carlson Justin Betteridge Richard C Wang Estevam R Hruschka Jr and Tom M Mitchell',\n",
       " 'Coupled semisupervised learning for information extraction',\n",
       " 'Jia Deng Wei Dong Richard Socher LiJia Li Kai Li and Li FeiFei',\n",
       " 'Imagenet A largescale hierarchical image database',\n",
       " 'Tilmann Gneiting and Adrian E Raftery',\n",
       " 'Strictly proper scoring rules prediction and estimation',\n",
       " 'Geoffrey Hinton Li Deng Dong Yu George E Dahl Abdelrahman Mohamed Navdeep Jaitly Andrew Senior Vincent Vanhoucke Patrick Nguyen Tara N Sainath et al',\n",
       " 'Deep neural networks for acoustic modeling in speech recognition The shared views of four research groups',\n",
       " 'Panagiotis G Ipeirotis Foster Provost Victor S Sheng and Jing Wang',\n",
       " 'Repeated labeling using multiple noisy labelers',\n",
       " 'Data Mining and Knowledge Discovery Srikanth Jagabathula Lakshminarayanan Subramanian and Ashwin Venkataraman',\n",
       " 'Crowdsourcing for book search evaluation impact of HIT design on comparative system ranking',\n",
       " 'David R Karger Sewoong Oh and Devavrat Shah',\n",
       " 'Iterative learning for reliable crowdsourcing systems',\n",
       " 'In Advances in neural information processing systems pages Qiang Liu Jian Peng and Alexander T Ihler',\n",
       " 'Vikas C Raykar Shipeng Yu Linda H Zhao Gerardo Hermosillo Valadez Charles Florin Luca Bogoni and Linda Moy',\n",
       " 'The Journal of Machine Nihar B Shah and Dengyong Zhou',\n",
       " 'Nihar B Shah Dengyong Zhou and Yuval Peres',\n",
       " 'Approval voting and incentives in crowdsourcing',\n",
       " 'Jeroen Vuurens Arjen P de Vries and Carsten Eickhoff',\n",
       " 'How much spam can you take',\n",
       " 'An analysis of crowdsourcing results to increase accuracy',\n",
       " 'Paul Wais Shivaram Lingamneni Duncan Cook Jason Fennell Benjamin Goldenberg Daniel Lubarov David Marin and Hari Simons',\n",
       " 'Towards building a highquality workforce with Mechanical Turk',\n",
       " 'Dengyong Zhou Qiang Liu John C Platt Christopher Meek and Nihar B Shah',\n",
       " 'Regularized minimax conditional entropy for crowdsourcing arXiv preprint \\x0cLearning with Symmetric Label Noise The  brendanvanrooyen adityamenon bobwilliamson nictacomau Convex potential minimisation is the de facto approach to binary classification',\n",
       " 'This ostensibly shows that convex losses are not SLNrobust',\n",
       " 'In this paper we propose a convex classificationcalibrated loss and prove that it is SLNrobust',\n",
       " 'The loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss',\n",
       " 'Experiments confirm the unhinged loss SLNrobustness is borne out in practice',\n",
       " 'Learning with symmetric label noise Binary classification is the canonical supervised learning problem',\n",
       " 'Our interest is in the more realistic scenario where the learner observes samples from some corruption D of D where labels have some constant probability of being flipped and the goal is still to perform well with respect to D',\n",
       " 'In this paper we propose a convex loss and prove that it is SLNrobust',\n",
       " 'The loss is a modification of the hinge loss where one does not clamp at zero thus we call it the unhinged loss',\n",
       " 'However establishing this classifiers strong SLNrobustness uniqueness thereof and its equivalence to a highly regularised SVM solution to our knowledge is novel',\n",
       " 'For a set S L S is the set of risks for all scorers in S',\n",
       " 'We now formalise this notion and review what is known about SLNrobust learners',\n",
       " 'Unfortunately a widely adopted class of learners is not SLNrobust as we will now see',\n",
       " 'This captures eg the linear SVM and logistic regression which are widely studied in theory and applied in practice',\n",
       " 'Then  Flin  is not SLNrobust',\n",
       " 'The fallout what learners are SLNrobust',\n",
       " 'In Appendix B we present evidence that some of these losses are in fact not SLNrobust when used with Flin ',\n",
       " 'The second approach is to consider suitably rich F that contains the Bayesoptimal scorer for D eg by employing a universal kernel',\n",
       " 'Then  RX  is SLNrobust',\n",
       " 'The first approach has a computational penalty as it requires optimising a nonconvex loss',\n",
       " 'The second approach has a statistical penalty as estimation rates with a rich F will require a larger sample size',\n",
       " 'Thus it appears that SLNrobustness involves a computationalstatistical tradeoff',\n",
       " 'However there is a variant of the first option pick a loss that is convex but not a convex potential',\n",
       " 'Such a loss would afford the computational and statistical advantages of minimising convex risks with linear scorers',\n",
       " 'We will show that there is a simpler loss that is convex and SLNrobust but is not in the class of convex potentials by virtue of being negatively unbounded',\n",
       " 'To derive this loss we first reinterpret robustness via a noisecorrection procedure',\n",
       " 'A noisecorrected loss perspective on SLNrobustness We now reexpress SLNrobustness to reason about optimal scorers on the same distribution but with two different losses',\n",
       " 'This will help characterise a set of strongly SLNrobust losses',\n",
       " 'The loss  is defined as follows',\n",
       " 'Characterising a stronger notion of SLNrobustness As the first step towards a stronger notion of robustness we rewrite with a slight abuse of notation Y S  L RD s where RD s is a distribution over labels and scores',\n",
       " 'Strong SLNrobustness can then be made precise as follows',\n",
       " 'We now reexpress strong SLNrobustness using a notion of order equivalence of loss pairs which simply requires that two losses order all distributions over labels and scores identically',\n",
       " 'It is thus not surprising that we can relate order equivalence to strong SLNrobustness of ',\n",
       " 'This connection now lets us exploit a classical result in decision theory about order equivalent losses being affine transformations of each other',\n",
       " 'We now return to our original goal which was to find a convex  that is SLNrobust for Flin and ideally more general function classes',\n",
       " 'Unfortunately it is evident that if  is convex nonconstant and bounded below by zero then it cannot possibly be admissible in this sense',\n",
       " 'But we now show that removing the boundedness restriction allows for the existence of a convex admissible loss',\n",
       " 'The loss has a number of attractive properties the most immediate being is its SLNrobustness',\n",
       " 'Further the following uniqueness property is not hard to show',\n",
       " 'That is up to scaling and translation unh is the only convex loss that is strongly SLNrobust',\n",
       " 'Returning to the case of linear scorers the above implies that unh  Flin  is SLNrobust',\n",
       " 'Intuitively this property allows the loss to offset the penalty incurred by instances that are misclassified with high margin by awarding a gain for instances that correctly classified with high margin',\n",
       " 'The unhinged loss is classification calibrated SLNrobustness is by itself insufficient for a learner to be useful',\n",
       " 'For example a loss that is uniformly zero is strongly SLNrobust but is useless as it is not classificationcalibrated',\n",
       " 'Fortunately the unhinged loss is classificationcalibrated as we now establish',\n",
       " 'We can sidestep this issue by restricting attention to bounded scorers so that unh is effectively bounded',\n",
       " 'Unhinged loss minimisation on corrupted distribution is consistent Using bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss',\n",
       " 'This shows classification consistency of unhinged loss minimisation on the corrupted distribution',\n",
       " 'Learning with the unhinged loss and kernels We now show that the optimal solution for the unhinged loss when employing regularisation and kernelised scorers has a simple form',\n",
       " 'This sheds further light on SLNrobustness and regularisation',\n",
       " 'Our result establishes that SLN robustness of the classifier holds without any assumptions on M ',\n",
       " 'We note several points relating to practical usage of the unhinged loss with kernelised scorers',\n",
       " 'Equivalence to a highly regularised SVM and other convex potentials There is an interesting equivalence between the unhinged solution and that of a highly regularised SVM',\n",
       " 'This has been noted in eg',\n",
       " 'With a wellspecified F eg with a universal kernel both the unhinged and square loss asymptotically recover the optimal classifier but the unhinged loss does not require a matrix inversion',\n",
       " 'SLNrobustness of unhinged loss empirical illustration We now illustrate that the unhinged loss SLNrobustness is empirically manifest',\n",
       " 'We reiterate that with high regularisation the unhinged solution is equivalent to an SVM and in the limit any classificationcalibrated loss solution',\n",
       " 'Thus we do not aim to assert that the unhinged loss is better than other losses but rather to demonstrate that its SLNrobustness is not purely theoretical',\n",
       " 'Grayed cells denote the best performer at that noise rate',\n",
       " 'By contrast both other losses suffer at even moderate noise rates',\n",
       " 'We proposed a convex classificationcalibrated loss proved that is robust to symmetric label noise SLNrobust showed it is the unique loss that satisfies a notion of strong SLNrobustness established that it is optimised by the nearest centroid classifier and showed that most convex potentials such as the SVM are also SLNrobust when highly regularised',\n",
       " 'NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program',\n",
       " 'The authors thank Cheng Soon Ong for valuable comments on a draft of this paper',\n",
       " 'Combining labeled and unlabeled data with cotraining',\n",
       " 'Vasil Denchev Nan Ding Hartmut Neven and SVN',\n",
       " 'Robust classification with adiabatic quantum optimization',\n",
       " 'A Probabilistic Theory of Pattern Recognition',\n",
       " 'Mathematical Statistics A Decision Theoretic Approach',\n",
       " 'Aritra Ghosh Naresh Manwani and P',\n",
       " 'Making risk minimization tolerant to label noise',\n",
       " 'Trevor Hastie Saharon Rosset Robert Tibshirani and Ji Zhu',\n",
       " 'The entire regularization path for the support vector machine',\n",
       " 'Efficient noisetolerant learning from statistical queries',\n",
       " 'Random classification noise defeats all convex potential boosters',\n",
       " 'IEEE Transactions on Cybernetics Hamed MasnadiShirazi Vijay Mahadevan and Nuno Vasconcelos',\n",
       " 'On the design of robust classifiers for computer vision',\n",
       " 'Random features for largescale kernel machines',\n",
       " 'Journal of Machine Learning Research Mark D Reid and Robert C Williamson',\n",
       " 'Information divergence and risk for binary experiments',\n",
       " 'On PAC learning using Winnow Perceptron and a Perceptronlike algorithm',\n",
       " 'A Hilbert space embedding for distributions',\n",
       " 'Sriperumbudur Kenji Fukumizu Arthur Gretton Gert R',\n",
       " 'Kernel choice and classifiability for RKHS embeddings of probability distributions',\n",
       " 'Learning SVMs from sloppily labeled data',\n",
       " 'Robert Tibshirani Trevor Hastie Balasubramanian Narasimhan and Gilbert Chu',\n",
       " 'Diagnosis of multiple cancer types by shrunken centroids of gene expression',\n",
       " 'This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations',\n",
       " 'In this paper we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions',\n",
       " 'We provide various interpretations of this result',\n",
       " 'For instance a relationship is proved between stability and data processing which reveals that algorithmic stability can be improved by postprocessing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning',\n",
       " 'In addition we establish a relationship between algorithmic stability and the size of the observation space which provides a formal justification for dimensionality reduction methods',\n",
       " 'Finally we connect algorithmic stability to the size of the hypothesis space which recovers the classical PAC result that the size complexity of the hypothesis space should be controlled in order to improve algorithmic stability One fundamental goal of any learning algorithm is to strike a right balance between underfitting and overfitting',\n",
       " 'In mathematical terms this is often translated into two separate objectives',\n",
       " 'First we would like the learning algorithm to produce a hypothesis that is reasonably consistent with the empirical evidence ie to have a small empirical risk',\n",
       " 'Second we would like to guarantee that the empirical risk training error is a valid estimate of the true unknown risk test error',\n",
       " 'The former condition protects against underfitting while the latter condition protects against overfitting',\n",
       " 'Then it is elementary to observe that the true risk Rtrue is bounded from above by the sum Remp  Rgen ',\n",
       " 'Hence by minimizing both the empirical risk underfitting and the generalization risk overfitting one obtains an inference procedure whose true risk is minimal',\n",
       " 'However the generalization risk is often impossible to deal with directly',\n",
       " 'Instead it is a common practice to bound it analyticaly so that we can establish conditions under which it is guaranteed to be small',\n",
       " 'By establishing conditions for generalization one hopes to design better learning algorithms that both perform well empirically and generalize well to novel observations in the future',\n",
       " 'However bounding the generalization risk is quite intricate because it can be approached from various angles',\n",
       " 'In this setting we have an observation space Z and a hypothesis space H',\n",
       " 'By imposing constraints on any of these three components one may be able to derive new generalization bounds',\n",
       " 'Given that different generalization bounds can be established by imposing constraints on any of Z H or L it is intriguing to ask if there exists a single view for generalization that ties all of these different components together',\n",
       " 'In this paper we answer this question in the affirmative by establishing that algorithmic stability alone is equivalent to uniform generalization',\n",
       " 'Informally speaking an inference process is said to generalize uniformly if the generalization risk vanishes uniformly across all bounded parametric loss functions at the limit of large training sets',\n",
       " 'A more precise definition will be presented in the sequel',\n",
       " 'We will show why constraints that are imposed on either H Z or L to improve uniform generalization can be interpreted as methods of improving the stability of the learning algorithm L',\n",
       " 'Our statement however is more general as it applies to all learning algorithms that fall under Vapniks general setting of learning well beyond uniform convergence',\n",
       " 'The rest of the paper is as follows',\n",
       " 'First we review the current literature on algorithmic stability generalization and learnability',\n",
       " 'Then we introduce key definitions that will be repeatedly used throughout the paper',\n",
       " 'Next we prove the central theorem which reveals that algorithmic stability is equivalent to uniform generalization and provide various interpretations of this result afterward',\n",
       " 'The two concepts are distinct from each other',\n",
       " 'As will be discussed in more details next whereas learnability is concerned with measuring the excess risk within a hypothesis space generalization is concerned with estimating the true risk',\n",
       " 'Note that L is implicitly a function of parameterized by H as well',\n",
       " 'Unlike learnability the question of generalization is concerned primarily with how representative the empirical risk Remp is to the true risk Rtrue ',\n",
       " 'By contrast it is not known whether algorithmic stability is necessary for generalization',\n",
       " 'However it is not known whether an appropriate notion of algorithmic stability can be defined that is both necessary and sufficient for generalization in Vapniks general setting of learning',\n",
       " 'In this paper we answer this question by showing that stability in the inference process is not only sufficient for generalization but it is in fact equivalent to uniform generalization which is a notion of generalization that is stronger than the one traditionally considered in the literature',\n",
       " 'To simplify the discussion we will always assume that all sets are countable including the observation space Z and the hypothesis space H',\n",
       " 'In addition we assume that all learning algorithms are invariant to permutations of the training set',\n",
       " 'Hence the order of training examples is irrelevant',\n",
       " 'In general random variables are denoted with capital letters instances of random variables are denoted with small letters and alphabets are denoted with calligraphic typeface',\n",
       " 'Also given two probability mass functions P and Q defined on the same alphabet A we will write hP Qi to denote the overlapping coefficient ie intersection between P and Q',\n",
       " 'To reiterate we have an observation space Z and a hypothesis space H',\n",
       " 'In this paper we allow the hypothesis H to be any summary statistic of the training set',\n",
       " 'It can be a measure of central tendency as in unsupervised learning or it can be a mapping from an input space to an output space as in supervised learning',\n",
       " 'In fact we even allow H to be a subset of the training set itself',\n",
       " 'The converse however is not true',\n",
       " 'Even though uniform generalization appears to be quite a strong condition at first sight a key contribution of this paper is to show that it is not a strong condition because it is equivalent to a simple condition namely algorithmic stability',\n",
       " 'Before we prove that algorithmic stability is equivalent to uniform generalization we introduce a probabilistic notion of mutual stability between two random variables',\n",
       " 'It measures how stable the distribution of Y is before and after observing an instance of X and vice versa',\n",
       " 'A small value of SX Y  means that the probability distribution of X or Y is heavily perturbed by a single observation of the other random variable',\n",
       " 'Perfect mutual stability is achieved when the two random variables are independent of each other',\n",
       " 'We define the stability of L by SL  inf Pz SH Ztrn  where the infimum is taken over all possible distributions of observations Pz',\n",
       " 'Note that the above definition of algorithmic stability is rather weak it only requires that the contribution of any single training example on the overall inference process to be more and more negligible as the sample size increases',\n",
       " 'In addition it is welldefined even if the learning algorithm is deterministic because the hypothesis H if it is a deterministic function of an entire training set of m observations remains a stochastic function of any individual observation',\n",
       " 'Before we do that we first state the following lemma',\n",
       " 'Now we are ready to state the main result of this paper',\n",
       " 'Here is an outline of the proof',\n",
       " 'Therefore algorithmic stability is sufficient for uniform generalization',\n",
       " 'Therefore algorithmic stability is also necessary for uniform generalization',\n",
       " 'Interpreting Algorithmic Stability and Uniform Generalization In this section we provide several interpretations of algorithmic stability and uniform generalization',\n",
       " 'This presents us with qualitative insights into the design of machine learning algorithms',\n",
       " 'This brings us to the following remark',\n",
       " 'We can improve the uniform generalization bound or equivalently algorithmic stability of a learning algorithm by postprocessing its inferred hypothesis H in a manner that is conditionally independent of the original training set given H',\n",
       " 'Postprocessing hypotheses is a common technique used in machine learning',\n",
       " 'By the data processing inequality such methods improve algorithmic stability and uniform generalization',\n",
       " 'Needless to mention better generalization does not immediately translate into a smaller true risk',\n",
       " 'This is because the empirical risk itself may increase when the inferred hypothesis is postprocessed independently of the original training set',\n",
       " 'As a result we can improve algorithmic stability by contaminating training examples with artificial noise prior to learning',\n",
       " 'We can improve the algorithmic stability of a learning algorithm by introducing artificial noise to training examples and applying the learning algorithm on the perturbed training set',\n",
       " 'By the data processing inequality such methods indeed improve algorithmic stability and uniform generalization',\n",
       " 'Algorithmic Stability and the Size of the Observation Space Next we look into how the size of the observation space Z influences algorithmic stability',\n",
       " 'A lazy learner is called lazy if its hypothesis is equivalent to the original training set in its information content',\n",
       " 'Hence no learning actually takes place',\n",
       " 'One example is instancebased learning when H  Sm ',\n",
       " 'Despite their simple nature lazy learners are useful in practice',\n",
       " 'They are useful theoretical tools as well',\n",
       " 'Therefore we can relate algorithmic stability uniform generalization to the size of the observation space by quantifying the algorithmic stability of lazy learners',\n",
       " 'Because the size of Z is usually infinite however we introduce the following definition of effective set size',\n",
       " 'In a countable space Z endowed with a probability mass function Pz the effective size of Z wrt',\n",
       " 'At one extreme if Pz is uniform over a finite alphabet Z then Ess Z Pz  Z',\n",
       " 'As proved next this notion of effective set size determines the rate of convergence of an empirical probability mass function to its true distribution when the distance is measured in the total variation sense',\n",
       " 'As a result it allows us to relate algorithmic stability to a property of the observation space Z',\n",
       " 'Let Z be a countable space endowed with a probability mass function Pz',\n",
       " 'Define PSm z to be the empirical probability mass function q induced by drawing samples uniformly at random from Sm ',\n",
       " 'Here is an outline of the proof',\n",
       " 'Needless to mention this is difficult to achieve in practice so the algorithmic stability of machine learning algorithms must be controlled in order to guarantee a good generalization from a few empirical observations',\n",
       " 'Similarly the uniform generalization bound can be improved by reducing the effective size of the observation space such as by using dimensionality reduction methods',\n",
       " 'Algorithmic Stability and the Complexity of the Hypothesis Space Finally we look into the hypothesis space and how it influences algorithmic stability',\n",
       " 'First we look into the role of the size of the hypothesis space',\n",
       " 'This is formalized in the following theorem',\n",
       " 'Then the following bound on algorithmic stability always holds where H is the Shannon entropy measured in nats ie using natural logarithms',\n",
       " 'In terms of algorithmic stability a learning algorithm will enjoy a high stability if the size of the hypothesis space is small',\n",
       " 'Next we relate algorithmic stability to the VapnikChervonenkis VC dimension',\n",
       " 'Despite the fact that the VC dimension is defined on binaryvalued functions whereas algorithmic stability is a functional of probability distributions there exists a connection between the two concepts',\n",
       " 'That is H splits Z into two disjoint sets the set of values in Z that are a posteriori less likely to have been present in the training set than before given that the inferred hypothesis is H and the set of all other values',\n",
       " 'The complexity richness of the induced concept class C is related to algorithmic stability via the VC dimension',\n",
       " 'Let dV C C be the VC dimension of C',\n",
       " 'In this paper we showed that a probabilistic notion of algorithmic stability was equivalent to uniform generalization',\n",
       " 'In informal terms a learning algorithm is called algorithmically stable if the impact of a single training example on the probability distribution of the final hypothesis always vanishes at the limit of large training sets',\n",
       " 'In other words the inference process never depends heavily on any single training example',\n",
       " 'If algorithmic stability holds then the learning algorithm generalizes well regardless of the choice of the parametric loss function',\n",
       " 'We also provided several interpretations of this result',\n",
       " 'For instance the relationship between algorithmic stability and data processing reveals that algorithmic stability can be improved by either postprocessing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning',\n",
       " 'In addition we established a relationship between algorithmic stability and the effective size of the observation space which provided a formal justification for dimensionality reduction methods',\n",
       " 'Lugosi A probabilistic theory of pattern recognition',\n",
       " 'BenDavid Understanding Machine Learning From Theory to Algorithms',\n",
       " 'Forsythe Massachusetts Institute of Technology Lincoln Laboratory We develop a sequential lowcomplexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown apriori',\n",
       " 'We present an easily computable closed form parametric expression for the conditional likelihood in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors',\n",
       " 'Motivated by largesample asymptotics we propose a novel adaptive lowcomplexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate',\n",
       " 'We further prove that in the largesample limit the conditional likelihood and data predictive distribution become asymptotically Gaussian',\n",
       " 'Traditional finite mixture models often suffer from overfitting or underfitting of data due to possible mismatch between the model complexity and amount of data',\n",
       " 'Thus model selection or model averaging is required to find the correct number of clusters or the model with the appropriate complexity',\n",
       " 'This requires significant computation for highdimensional data sets or large samples',\n",
       " 'However these methods can exhibit slow convergence and their convergence can be tough to detect',\n",
       " 'These approaches can take a significant computational effort even for moderate sized data sets',\n",
       " 'For largescale data sets and lowlatency applications with streaming data there is a need for inference algorithms that are much faster and do not require multiple passes through the data',\n",
       " 'In this work we focus on lowcomplexity algorithms that adapt to each sample as they arrive making them highly scalable',\n",
       " 'The algorithm is called sequential updating and greedy search SUGS and each iteration is composed of a greedy selection step and a posterior update step',\n",
       " 'While the basic idea behind ASUGS is directly related to the greedy approach of SUGS the main contribution is a novel lowcomplexity stable method for choosing the concentration parameter adaptively as new data arrive which greatly improves the clustering performance',\n",
       " 'We derive an upper bound on the number of classes logarithmic in the number of samples and further prove that the sequence of concentration parameters that results from this adaptive design is almost bounded',\n",
       " 'We finally prove that the conditional likelihood which is the primary tool used for Bayesianbased online clustering is asymptotically Gaussian in the largesample limit implying that the clustering part of ASUGS asymptotically behaves as a Gaussian classifier',\n",
       " 'Experiments show that our method outperforms other stateoftheart methods for online learning of DPMMs',\n",
       " 'The paper is organized as follows',\n",
       " 'Here the nonparametric nature of the Dirichlet process manifests itself as modeling mixture models with countably infinite components',\n",
       " 'The online sequential updating and greedy search SUGS algorithm is summarized next for completeness',\n",
       " 'The algorithm sequentially allocates observations yi to classes based on maximizing the conditional posterior probability',\n",
       " 'Furthermore in the streaming data setting where no estimate on the data complexity exists it is impractical to perform crossvalidation',\n",
       " 'The selection step may be implemented by sampling the probability mass function qh ',\n",
       " 'The posterior update step can be efficiently performed by updating the hyperparameters as a function of the streaming data for the case of conjugate distributions',\n",
       " 'Sequential Inference under Unknown Mean  Unknown Covariance We consider the general case of an unknown mean and covariance for each class',\n",
       " 'The follow a normalWishart joint distribution',\n",
       " 'To calculate the class posteriors the conditional likelihoods of yi given assignment to class h and the previous class assignments need to be calculated first',\n",
       " 'The form of this recursive computation of the hyperparameters is derived in Appendix A',\n",
       " 'A detailed mathematical derivation of this conditional likelihood is included in Appendix B',\n",
       " 'Choosing the innovation class pushes mass toward infinity while choosing any other class pushes mass toward zero',\n",
       " 'Thus there is a possibility that the innovation probability grows in a undesired manner',\n",
       " 'In our experiments this approximation is very accurate',\n",
       " 'The asymptotic behavior of rn and related variables is described in the following theorem',\n",
       " 'This follows from the strong law of large numbers as the updates are recursive implementations of the sample mean and sample covariance matrix',\n",
       " 'The experiments show the value of adaptation of the Dirichlet concentration parameter for online clustering and parameter estimation',\n",
       " 'ASUGSPM performs best and identifies the correct number of clusters and their parameters',\n",
       " 'The ASUGSbased approaches achieve a higher loglikelihood than SVAbased approaches asymptotically',\n",
       " 'ASUGSPM achieves the highest loglikelihood and has the lowest asymptotic variance on the number of classes',\n",
       " 'We applied the online nonparametric Bayesian methods for clustering image data',\n",
       " 'Although both SVA and ASUGS methods have similar computational complexity and use decisions and information obtained from processing previous samples in order to decide on class innovations the mechanics of these methods are quite different',\n",
       " 'Furthermore SVA updates the parameters of all the components at each iteration in a weighted fashion while ASUGS only updates the parameters of the mostlikely cluster thus minimizing leakage to unrelated components',\n",
       " 'This is critical for large data sets or streaming applications because crossvalidation would be required to set \\x0f appropriately',\n",
       " 'We observe higher loglikelihoods and better numerical stability for ASUGSbased methods in comparison to SVA',\n",
       " 'We developed a fast online clustering and parameter estimation algorithm for Dirichlet process mixtures of Gaussians capable of learning in a single data pass',\n",
       " 'Motivated by largesample asymptotics we proposed a novel lowcomplexity datadriven adaptive design for the concentration parameter and showed it leads to logarithmic growth rates on the number of classes',\n",
       " 'Through experiments on synthetic and real data sets we show our method achieves better performance and is as fast as other stateoftheart online learning DPMM methods',\n",
       " 'Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems',\n",
       " 'Variational Inference for Dirichlet Process Mixtures',\n",
       " 'Fast Search for Dirichlet Process Mixture Models',\n",
       " 'Bayesian Density Estimation and Inference using Mixtures',\n",
       " 'Particle Filters for Mixture Models with an Uknown Number of Components',\n",
       " 'Kurihara K Welling M and Vlassis N',\n",
       " 'Online learning of nonparametric mixture models via sequential variational approximation',\n",
       " 'Markov chain sampling methods for Dirichlet process mixture models',\n",
       " 'A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation',\n",
       " 'The Variational Approximation for Bayesian Inference',\n",
       " 'Fast Bayesian Inference in Dirichlet Process Mixture Models',\n",
       " 'Thermostat for LargeScale Bayesian Sampling Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning',\n",
       " 'The Markov chain Monte Carlo procedures that are used are often discretetime analogues of associated stochastic differential equations SDEs',\n",
       " 'These SDEs are guaranteed to leave invariant the required posterior distribution',\n",
       " 'An area of current research addresses the computational benefits of stochastic gradient methods in this setting',\n",
       " 'Existing techniques rely on estimating the variance or covariance of the subsampling error and typically assume constant variance',\n",
       " 'In this article we propose a covariancecontrolled adaptive Langevin thermostat that can effectively dissipate parameterdependent noise while maintaining a desired target distribution',\n",
       " 'The proposed method achieves a substantial speedup over popular alternative schemes for largescale machine learning applications',\n",
       " 'In machine learning applications direct sampling with the entire largescale dataset is computationally infeasible',\n",
       " 'However it is difficult to accommodate the additional diffusion term in practice',\n",
       " 'Despite the substantial interest generated by these methods the mathematical foundation for stochastic gradient methods has been incomplete',\n",
       " 'SGNHT methods are designed based on the assumption of constant noise variance',\n",
       " 'In this article we propose a covariancecontrolled adaptive Langevin CCAdL thermostat that can handle parameterdependent noise improving both robustness and reliability in practice and which can effectively speed up the convergence to the desired invariant distribution in largescale machine learning applications',\n",
       " 'The rest of the article is organized as follows',\n",
       " 'Assuming the data are independent and identically distributed iid the logarithm of the likelihood where N is the size of the entire dataset',\n",
       " 'However as already mentioned it is computationally infeasible to deal with the entire largescale dataset at each timestep as would typically be required in MCMC and HMC methods',\n",
       " 'Therefore it is essential to have an approach that can handle parameterdependent noise',\n",
       " 'In the following section we propose a covariancecontrolled thermostat that can effectively dissipate parameterdependent noise while maintaining the target stationary distribution',\n",
       " 'In fact in that case it is not clear whether or not there exists an invariant distribution at all \\x0cIn order to construct a stochasticdynamical system that preserves the canonical distribution we suggest adding a suitable damping viscous term to effectively dissipate the parameterdependent gradient noise',\n",
       " 'In numerical experiments we have found that simply using the diagonal of the covariance matrix at significantly reduced computational cost works quite well in CCAdL',\n",
       " 'One may attempt to use a large value of the effective friction A andor a small stepsize h',\n",
       " 'As already mentioned estimating the full covariance matrix is computationally infeasible in high dimension',\n",
       " 'Note that this is a simple first order in terms of the stepsize algorithm',\n",
       " 'In the following section we compare the newly established CCAdL method with SGHMC and SGNHT on various machine learning tasks to demonstrate the benefits of CCAdL in Bayesian sampling with a noisy gradient',\n",
       " 'Bayesian Inference for Gaussian Distribution We first compare the performance of the newly established CCAdL method with SGHMC and SGNHT for a simple task using synthetic data ie',\n",
       " 'Bayesian inference of both the mean and variance of a onedimensional normal distribution',\n",
       " 'The peak region is highlighted in the inset',\n",
       " 'Since the dimensionality of this problem is not that high a full covariance estimation was used for CCAdL',\n",
       " 'The true reference distribution was obtained by a sufficiently long run of standard HMC',\n",
       " 'Again CCAdL shows much better performance than SGHMC and SGNHT',\n",
       " 'Magenta circle is the true reference posterior mean obtained from standard HMC and crosses represent the sample means computed from various methods',\n",
       " 'Note that the contour of SGHMC is well beyond the scale of the plot especially in the large stepsize regime in which case we do not include it here',\n",
       " 'We selected the number of hidden units using crossvalidation to achieve their best results',\n",
       " 'Since the dimension of parameters Nd  is relatively high we used only diagonal covariance matrix estimation for CCAdL to significantly reduce the computational cost ie estimating the variance only along each dimension',\n",
       " 'It can be observed that SGHMC and SGNHT only work well with a large value of the effective friction A which corresponds to a strong random walk effect and thus slows down the convergence',\n",
       " 'On the contrary CCAdL works \\x0creliably much better than the other two in a wide range of A and more importantly in the large stepsize regime which speeds up the convergence rate in relation to the computational work performed',\n",
       " 'In this article we have proposed a novel CCAdL formulation that can effectively dissipate parameterdependent noise while maintaining a desired invariant distribution',\n",
       " 'CCAdL combines ideas of SGHMC and SGNHT from the literature but achieves significant improvements over each of these methods in practice',\n",
       " 'The additional error introduced by covariance estimation is expected to be small in a relative sense ie substantially smaller than the error arising from the noisy gradient',\n",
       " 'Our findings have been verified in largescale machine learning applications',\n",
       " 'In particular we have consistently observed that SGHMC relies on a small stepsize h and a large friction A which significantly reduces its usefulness in practice as discussed',\n",
       " 'The techniques presented in this article could be of use in more general settings of largescale Bayesian sampling and optimization which we leave for future work',\n",
       " 'A naive nonsymmetric splitting method has been applied for CCAdL for fair comparison in this article',\n",
       " 'Long time accuracy of LieTrotter splitting methods for Langevin dynamics',\n",
       " 'Bayesian posterior sampling via stochastic gradient Fisher scoring',\n",
       " 'Bayesian sampling using stochastic gradient thermostats',\n",
       " 'Understanding Molecular Simulation From Algorithms to Applications Second Edition',\n",
       " 'A generalized guided Monte Carlo algorithm',\n",
       " 'Adaptive stochastic methods for sampling driven molecular systems',\n",
       " 'Classification using discriminative restricted Boltzmann machines',\n",
       " 'Rational construction of stochastic numerical methods for molecular sampling',\n",
       " 'The computation of averages from equilibrium and nonequilibrium Langevin molecular dynamics',\n",
       " 'Adaptive thermostats for noisy gradient systems',\n",
       " 'Equation of state calculations by fast computing machines',\n",
       " 'A unified formulation of the constant temperature molecular dynamics methods',\n",
       " 'Monte Carlo Statistical Methods Second Edition',\n",
       " 'Bayesian learning via stochastic gradient Langevin dynamics',\n",
       " 'We propose a robust portfolio optimization approach based on quantile statistics',\n",
       " 'The proposed method is robust to extreme events in asset returns and accommodates large portfolios under limited historical data',\n",
       " 'Specifically we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns',\n",
       " 'The theory does not rely on higher order moment assumptions thus allowing for heavytailed asset returns',\n",
       " 'Moreover the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data',\n",
       " 'The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data',\n",
       " 'Our work extends existing ones by achieving robustness in high dimensions and by allowing serial dependence',\n",
       " 'Therefore many studies focus on the global minimum variance GMV formulation which only involves estimating the covariance matrix of the asset returns',\n",
       " 'Estimating the covariance matrix of asset returns is challenging due to the high dimensionality and heavytailedness of asset return data',\n",
       " 'Specifically the number of assets under management is usually much larger than the sample size of exploitable historical data',\n",
       " 'On the other hand extreme events are typical in financial asset prices leading to heavytailed asset returns',\n",
       " 'These estimators are commonly based on the sample covariance matrix subGaussian tail assumptions are required to guarantee consistency',\n",
       " 'For heavytailed data robust estimators of covariance matrices are desired',\n",
       " 'These estimators are specifically designed for data with very low dimensions and large sample sizes',\n",
       " 'However although OGK is computationally tractable in high dimensions consistency is only guaranteed under fixed dimension',\n",
       " 'The shrunken Tylors M estimator involves iteratively inverting large matrices',\n",
       " 'Moreover its consistency is only guaranteed when the dimension is in the same order as the sample size',\n",
       " 'The aforementioned robust estimators are analyzed under independent data points',\n",
       " 'Their performance under time series data is questionable',\n",
       " 'Our contributions are in three aspects',\n",
       " 'First we show that the proposed method accommodates high dimensional data by allowing the dimension to scale exponentially with sample size',\n",
       " 'Secondly we verify that consistency of the proposed method is achieved without any tail conditions thus allowing for heavytailed asset return data',\n",
       " 'Thirdly we consider weakly dependent time series and demonstrate how the degree of dependence affects the consistency of the In this section we introduce the notation system and provide a review on the grossexposure constrained portfolio optimization that will be exploited in this paper',\n",
       " 'We write X  Y if X and Y are identically distributed',\n",
       " 'In this section we introduce the quantilebased portfolio optimization approach',\n",
       " 'Here Ze is an independent copy of Z',\n",
       " 'In practice RQ is b Q onto the cone unknown and has to be estimated',\n",
       " 'For convexity of the risk function we project R e Q  argminR  st',\n",
       " 'We summarize the e Q  we formulate the empirical robust portfolio algorithm in the supplementary material',\n",
       " 'First in order that the portfolio optimization is convex and well conditioned a positive definite matrix with lower bounded eigenvalues is needed',\n",
       " 'OGK induces positive definiteness by reestimating the eigenvalues using the variances of the principal components',\n",
       " 'In this section we provide theoretical analysis of the proposed portfolio optimization approach',\n",
       " 'Thus the number of assets under management is allowed to scale exponentially with sample size T ',\n",
       " 'The effect of serial dependence P on the rate of convergence is characterized by C\\x0f ',\n",
       " 'The number of assets d is allowed to scale exponentially with sample size T ',\n",
       " 'Moreover the rate of convergence does not rely on any tail conditions on the distribution of the asset For the rest of this section we build the connection between the proposed robust portfolio optimization and its momentbased counterpart',\n",
       " 'Here S  AA has rank r',\n",
       " 'Commonly used elliptical distributions include Gaussian distribution and tdistribution',\n",
       " 'Then we have for some constant m only depending on the distribution of X',\n",
       " 'In this section we investigate the empirical performance of the proposed portfolio optimization approach',\n",
       " 'The proposed portfolio optimization approach QNE is compared with three competitors',\n",
       " 'After the factor loadings are generated they are fixed as parameters throughout the simulations',\n",
       " 'Again these standard deviations are fixed as parameters once they are generated',\n",
       " 'Here the matching rate is defined as follows',\n",
       " 'Thus we conclude that QNE is robust to heavy tails in both risk minimization and asset selection',\n",
       " 'Since the true covariance matrix of the stock returns is unknown we adopt the Sharpe ratio for evaluating the performances of the portfolios',\n",
       " 'We observe that QNE achieves the largest Sharpe ratios under all values of the grossexposure constant indicating the lowest risks under the same returns or equivalently the highest returns under the same risk',\n",
       " 'In this paper we propose a robust portfolio optimization framework building on a quantilebased scatter matrix',\n",
       " 'We obtain nonasymptotic rates of convergence for the scatter matrix estimators and the risk of the estimated portfolio',\n",
       " 'The relations of the proposed framework with its momentbased The main contribution of the robust portfolio optimization approach lies in its robustness to heavy tails in high dimensions',\n",
       " 'If d \\x1c n statistical error diminishes rapidly with increasing n',\n",
       " 'However when d \\x1d n statistical error may scale rapidly with dimension',\n",
       " 'In this paper based on quantile statistics we achieve consistency for portfolio risk without assuming any tail conditions while allowing d to scale nearly exponentially with n',\n",
       " 'Another contribution of his work lies in the theoretical analysis of how serial dependence may affect consistency of the estimation',\n",
       " 'On the sensitivity of meanvarianceefficient portfolios to changes in asset means some analytical and computational results',\n",
       " 'The effect of errors in means variances and covariances on optimal portfolio choice',\n",
       " 'On estimating the expected return on the market An exploratory investigation',\n",
       " 'High dimensional covariance matrix estimation using a factor model',\n",
       " 'Forecasting using principal components from a large number of predictors',\n",
       " 'Statistical analysis of factor models of high dimension',\n",
       " 'Large covariance estimation by thresholding principal orthogonal complements',\n",
       " 'Improved estimation of the covariance matrix of stock returns with an application to portfolio selection',\n",
       " 'A wellconditioned estimator for largedimensional covariance matrices',\n",
       " 'Honey I shrunk the sample covariance matrix',\n",
       " 'Robust estimates of location and dispersion for highdimensional datasets',\n",
       " 'Robust estimates residuals and outlier detection with multiresponse data',\n",
       " 'Robust shrinkage estimation of highdimensional covariance matrices',\n",
       " 'Large dimensional analysis and optimization of robust shrinkage covariance matrix estimators',\n",
       " 'Risk reduction in large portfolios Why imposing the wrong constraints helps',\n",
       " 'Vast portfolio selection with grossexposure constraints',\n",
       " 'Alternatives to the median absolute deviation',\n",
       " 'Solving the matrix nearness problem in the maximum norm by applying a projection and contraction method',\n",
       " 'Quantile regression for analyzing heterogeneity in ultrahigh dimension',\n",
       " 'Optimal rates of convergence for covariance matrix estimation',\n",
       " 'Tail dependence for elliptically contoured distributions',\n",
       " 'Handbook of Heavy Tailed Distributions in Finance',\n",
       " 'Fattailed and Skewed Asset Return Distributions Implications for Risk Management Portfolio Selection and Option Pricing',\n",
       " 'Estimating high dimensional covariance matrices and its applications',\n",
       " 'Statistics for Highdimensional Data Methods Theory and \\x0cLogarithmic Time Online Multiclass prediction Courant Institute of Mathematical Sciences We study the problem of multiclass classification with an extremely large number of classes k with the goal of obtaining train and test time complexity logarithmic in the number of classes',\n",
       " 'We develop topdown tree construction approaches for constructing logarithmic depth trees',\n",
       " 'On the theoretical front we formulate a new objective function which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure in terms of class labels and balanced',\n",
       " 'We demonstrate that under favorable conditions we can construct logarithmic depth trees that have leaves with low label entropy',\n",
       " 'However the objective function at the nodes is challenging to optimize computationally',\n",
       " 'We address the empirical problem with a new online decision tree construction procedure',\n",
       " 'Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches which makes it a plausible method in computationally constrained largek applications',\n",
       " 'The central problem of this paper is computational complexity in a setting where the number of classes k for multiclass prediction is very large',\n",
       " 'Such problems occur in natural language Which translation is best search What result is best and detection Who is that tasks',\n",
       " 'In essence any multiclass classification algorithm must uniquely specify the bits of all labels that it predicts correctly on',\n",
       " 'The goal of logarithmic in k complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels with one label per leaf',\n",
       " 'While this hierarchy is sometimes available through prior knowledge in many scenarios it needs to be learned as well',\n",
       " 'This naturally leads to a partition problem which arises at each node in the hierarchy',\n",
       " 'Definitions of purity vary but canonical examples are the number of labels remaining in each subset or softer notions such as the average Shannon entropy of the class labels',\n",
       " 'Despite resulting in a classifier this problem is fundamentally different from standard binary classification',\n",
       " 'The partition problem is fundamentally nonconvex Throughout the paper by logarithmic time we mean logarithmic time per example',\n",
       " 'The choice of partition matters in problem dependent ways',\n",
       " 'For example consider examples on a line with label i at position i and threshold classifiers',\n",
       " 'In the multiclass setting it is desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes and thereby decrease the computational complexity',\n",
       " 'The main theoretical contribution of this work is to establish a boosting algorithm for learning trees with Ok nodes and Olog k depth thereby addressing the goal of logarithmic time train and test complexity',\n",
       " 'As in all boosting results performance is critically dependent on the quality of the weak learner supporting intuition that we need sufficiently rich partitioners at nodes',\n",
       " 'The approach uses a new objective for decision tree learning which we optimize at each node of the tree',\n",
       " 'Whenever there are representational constraints on partitions such as linear classifiers finding a strong partition function requires an efficient search over this set of classifiers',\n",
       " 'As the number of class choice of partition rather than assuming that labels grows the problem becomes harder and the one was handed to us',\n",
       " 'Does there exist a purity criterion amenable to a gradient descent apLOMtree becomes more dominant proach',\n",
       " 'The precise objective studied in theory fails this test due to its discrete nature and even natural approximations are challenging to tractably optimize under computational constraints',\n",
       " 'As a result we use the theoretical objective as a motivation and construct a new Logarithmic Online Multiclass Tree LOMtree algorithm for empirical Creating a tree in an online fashion creates a new class of problems',\n",
       " 'What if some node is initially created but eventually proves useless because no examples go to it',\n",
       " 'At best this results in a wasteful solution while in practice it starves other parts of the tree which need representational complexity',\n",
       " 'To deal with this we design an efficient process for recycling orphan nodes into locations where they are needed and prove that the number of times a node is recycled is at most logarithmic in the number of examples',\n",
       " 'We find that under constrained training times this approach is quite effective compared to all baselines while dominating other Olog k train time approaches',\n",
       " 'To the best of our knowledge the splitting criterion the boosting statement the LOMtree algorithm the swapping guarantee and the experimental results are all new here',\n",
       " 'Only a few authors address logarithmic time training',\n",
       " 'The Filter tree does not address the partition problem as we do here which as shown in our experimental section is often helpful',\n",
       " 'Quite a few authors have addressed logarithmic testing time while allowing training time to be Ok or worse',\n",
       " 'While these approaches are intractable on our larger scale problems we describe them here for context',\n",
       " 'The authors decouple the learning processes of coding matrix and bit predictors and use probabilistic decoding to decode the optimal class label',\n",
       " 'Their approach in general requires Ok running time to decode since in essence the fit of each label to the predictions must be checked and there are Ok labels',\n",
       " 'Both approaches however have Ok training time',\n",
       " 'Decision trees are naturally structured to allow logarithmic time prediction',\n",
       " 'Traditional decision trees often have difficulties with a large number of classes because their splitting criteria are not wellsuited to the large class setting',\n",
       " 'The reduction approach we use for optimizing partitions implicitly optimizes a differential objective',\n",
       " 'In this section we describe the essential elements of the approach and outline the theoretical properties of the resulting framework',\n",
       " 'We employ a hierarchical approach for learning a multiclass decision tree structure training this structure in a topdown fashion',\n",
       " 'The overall objective is to learn a tree of depth Olog k where each node in the tree consists of a classifier from H',\n",
       " 'When we reach a leaf we predict according to the label with the highest frequency amongst the examples reaching that leaf',\n",
       " 'Further in the paper we skip index n whenever it is clear from the context that we consider a fixed tree \\x0cIn the interest of computational complexity we want to encourage the number of examples going to the left and right to be fairly balanced',\n",
       " 'For good statistical accuracy we want to send examples of class i almost exclusively to either the left or the right subtree thereby refining the purity of the class distributions at subsequent levels in the tree',\n",
       " 'The purity of a tree node is therefore a measure of whether the examples of each class reaching the node are then mostly sent to its one child node pure split or otherwise to both children impure split',\n",
       " 'This seems unsuitable with some of the more standard decision tree objectives such as Shannon or Gini entropy which leads us to design a new objective',\n",
       " 'We aim to maximize the objective Jh to obtain high quality partitions',\n",
       " 'Intuitively the objective encourages the fraction of examples going to the right from class i to be substantially different from the background fraction for each class i',\n",
       " 'We now make these intuitions more formal',\n",
       " 'We now define a similar definition for the balancedness of a split',\n",
       " 'We want an objective to achieve its optimum for simultaneously pure and balanced split',\n",
       " 'Our algorithm could also be implemented as batch or streaming where in case of the latter one can for example make one pass through the data per every tree level however for massive datasets making multiple passes through the data is computationally costly further justifying the need for an online approach',\n",
       " 'Quality of the entire tree The above section helps us understand the quality of an individual split produced by effectively maximizing Jh',\n",
       " 'We next reason about the quality of the entire tree as we add more and more nodes',\n",
       " 'We measure the quality of trees using the average entropy over all the leaves in the tree and track the decrease of this entropy as a function of the number of nodes',\n",
       " 'We adopt the weak learning framework',\n",
       " 'Under this assumption one can use the new decision tree approach to drive the error below any threshold',\n",
       " 'The analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight and choose to split it into two children',\n",
       " 'Let n be the heaviest leaf at time t',\n",
       " 'Consider splitting it to two children',\n",
       " 'The contribution of node n to the tree entropy changes after it splits',\n",
       " 'This implies that the larger the objective Jhn  is at time t the larger the entropy reduction ends up being which further reinforces intuitions to maximize J',\n",
       " 'In general it might not be possible to find any hypothesis with a large enough objective Jhn  to guarantee sufficient progress at this point so we appeal to a weak learning assumption',\n",
       " 'We next explain our empirical approach for maximizing the relaxed objective',\n",
       " 'The empirical estimates of the expectations can be easily stored and updated online in every tree node',\n",
       " 'During training the algorithm assigns a unique label to each node of the tree which is currently a leaf',\n",
       " 'This is the label with the highest frequency amongst the examples reaching that leaf',\n",
       " 'The test example is then labeled with the label assigned to the leaf that this example descended to',\n",
       " 'The stopping criterion for expanding the tree is when the number of nonleaf nodes reaches a threshold T ',\n",
       " 'Consider a scenario where the current training example descends to leaf j',\n",
       " 'The leaf can split create two children if the examples that reached it in the past were coming from at least two different The smallest leaf is the one with the smallest total number of data points reaching it in the past',\n",
       " 'PARENT v LEFT v and RIGHT v denote resp the parent and the left and right child of node v',\n",
       " 'GRANDPA v and SIBLING v denote respectively the grandparent of node v and the sibling of node v ie the node which has the same parent as v',\n",
       " 'We also refer to this prediction value as the score in this section',\n",
       " 'Left before the swap right after the swap classes',\n",
       " 'However if the number of nonleaf nodes of the tree reaches threshold T  no more nodes can be expanded and thus j cannot create children',\n",
       " 'Since the tree construction is done online some nodes created at early stages of training may end up useless because no examples reach them later on',\n",
       " 'This prevents potentially useful splits such as at leaf j',\n",
       " 'The general idea behind node recycling is to allow nodes to split if a certain condition is met',\n",
       " 'The LOMtree algorithm has statistical performance close to more common Ok approaches',\n",
       " 'Where computationally feasible we also compared with a oneagainstall classifier OAA as a representative Ok approach',\n",
       " 'We used The details of the source of each dataset are provided in the Supplementary material \\x0clinear regressors',\n",
       " 'Timewise LOMtree significantly outperforms OAA due to building only closeto logarithmic depth trees',\n",
       " 'The improvement in the training time increases with the number of classes in the classification problem',\n",
       " 'The time advantage of LOMtree comes with some loss of statistical accuracy with respect to OAA where OAA is tractable',\n",
       " 'We conclude that LOMtree significantly closes the gap between other logarithmic time methods and OAA making it a plausible approach in computationally constrained largek applications',\n",
       " 'The LOMtree algorithm reduces the multiclass problem to a set of binary problems organized in a tree structure where the partition in every tree node is done by optimizing a new partition criterion online',\n",
       " 'The criterion guarantees pure and balanced splits leading to logarithmic training and testing time for the tree classifier',\n",
       " 'We provide theoretical justification for our approach via a boosting statement and empirically evaluate it on multiple multiclass datasets',\n",
       " 'Empirically we find that this is the best available logarithmic time approach for multiclass classification problems',\n",
       " 'Note however that the mechanics of testing datastes are much easier  one can simply test with effectively untrained parameters on a few examples to measure the test speed thus the perexample test time for OAA on ImageNet and ODP is provided',\n",
       " 'Also to the best of our knowledge there exist no stateoftheart results of the OAA performance on these datasets published in the literature',\n",
       " 'On the boosting ability of topdown decision tree learning algorithms',\n",
       " 'Conditional probability tree estimation analysis and algorithms',\n",
       " 'Label embedding trees for large multiclass tasks',\n",
       " 'Fast and balanced Efficient label tree learning for large scale object recognition',\n",
       " 'Sparse output coding for largescale visual recognition',\n",
       " 'Least squares revisited Scalable approaches for multiclass prediction',\n",
       " 'Guessaverse loss functions for costsensitive multiclass boosting',\n",
       " 'Multilabel learning with millions of labels Recommending advertiser bid phrases for web pages',\n",
       " 'Fastxml A fast accurate and stable treeclassifier for extreme multilabel learning',\n",
       " 'Largescale multilabel learning with missing labels',\n",
       " 'Support vector machines classification with a very largescale taxonomy',\n",
       " 'Refined experts improving classification in large taxonomies',\n",
       " 'Entanglement and differentiable information gain maximization',\n",
       " 'Online learning and online convex optimization',\n",
       " 'Introductory lectures on convex optimization  a basic course',\n",
       " 'Imagenet A largescale hierarchical image \\x0cPlanar Ultrametrics for Image Segmentation We study the problem of hierarchical clustering on planar graphs',\n",
       " 'We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions',\n",
       " 'We apply our algorithm to the problem of hierarchical image segmentation',\n",
       " 'We formulate hierarchical image segmentation from the perspective of estimating an ultrametric distance over the set of image pixels that agrees closely with an input set of noisy pairwise distances',\n",
       " 'Thresholding an ultrametric immediately yields a partition into sets whose diameter is less than the given threshold',\n",
       " 'Varying this distance threshold naturally produces a hierarchical clustering in which clusters at high thresholds are composed of clusters at lower thresholds',\n",
       " 'Finding an ultrametric imposes the additional constraint that these multicuts are hierarchically consistent across different thresholds',\n",
       " 'We focus on the case where the input distances are specified by a planar graph',\n",
       " 'The paper is organized as follows',\n",
       " 'We first introduce the closest ultrametric problem and the relation between multicuts and ultrametrics',\n",
       " 'For notational simplicity in the remainder of the paper we frequently omit the dependence on G which is given as a fixed input',\n",
       " 'Pairs u v that do not correspond to an We assume by convention that edge in the original graph can still be assigned a unique distance based on the coarsest level l at which they lie in different connected components of the cut specified by X l ',\n",
       " 'Instead we develop a column generation approach tailored for planar graphs that allows for efficient and accurate approximate inference',\n",
       " 'The Cut Cone and Planar Multicuts Consider a partition of a planar graph into two disjoint sets of nodes',\n",
       " 'We denote the space of indicator vectors corresponding to such twoway cuts by CUT',\n",
       " 'A cut may yield more than two connected components but it can not produce every possible multicut eg it can not split a triangle of three nodes into three separate components',\n",
       " 'Since any multicut can be expressed as a superposition of cuts the cut cone is identical to the conic hull of MCUT',\n",
       " 'However this constraint is too rigid when Z does not include all possible cuts',\n",
       " 'In practice these dual constraints turn out to be essential for efficient optimization and constitute the core contribution of this paper',\n",
       " 'Solving the Dual via Cutting Planes The chief complexity of the dual LP is contained in the constraints including Z which encodes nonnegativity of an exponential number of cuts of the graph represented by the columns of Z',\n",
       " 'If this cut has nonnegative weight then all the constraints are satisfied otherwise we add the corresponding cut indicator vector as an additional column of Z',\n",
       " 'Unlike the multicut problem finding a twoway cut in a planar graph can be solved exactly by a reduction to minimumweight perfect matching',\n",
       " 'Values plotted are the gap between the bound and the best lowerbound computed at termination for a given problem instance',\n",
       " 'This relative gap is averaged over problem instances which have not yet converged at a given time point',\n",
       " 'Let the number of connected components of z l be denoted M ',\n",
       " 'For each of the M components then we add one column to Z corresponding to the cut that isolates that connected component from the rest',\n",
       " 'This allows more flexibility in representing the final optimum multicut as superpositions of these components',\n",
       " 'The subroutine isocutsz l  computes the set of cuts that isolate each connected component of z l ',\n",
       " 'We then repair uncut any cut edges that lie inside a connected component',\n",
       " 'Black circles indicate thresholds used in the closest UM optimization b Anytime performance Fmeasure on the BSDS benchmark as a function of runtime',\n",
       " 'Finally we weighted edges proportionally to the length of the corresponding boundary in the image',\n",
       " 'UCM performs agglomerative clustering of superpixels and assigns the lengthweighted averaged gP b value as the distance between each pair of merged regions',\n",
       " 'While UCM was not explicitly designed to find the closest ultrametric it provides a strong baseline for hierarchical clustering',\n",
       " 'We found the upperbound given by the cost of the decoded integer solution and the lowerbound estimated by the dual LP are very close',\n",
       " 'In the second image hierarchical segmentation UM better preserves semantic parts of the two birds while correctly merging the background regions',\n",
       " 'In terms of segmentation accuracy UM performs nearly identically to the state of the art UCM algorithm with some small gains in the highprecision regime',\n",
       " 'It is worth noting that the BSDS benchmark does not provide strong penalties for small leaks between two segments when the total number of boundary pixels involved is small',\n",
       " 'Our algorithm may find strong application in domains where the local boundary signal is noisier eg biological imaging or when undersegmentation is more heavily penalized',\n",
       " 'While our cuttingplane approach is slower than agglomerative clustering it is not necessary to wait for convergence in order to produce high quality results',\n",
       " 'We found that while the upper and lower bounds decrease as a function of time the clustering performance as measured by precisionrecall is often nearly optimal after only ten seconds and remains stable',\n",
       " 'Importance of enforcing hierarchical constraints Although independently finding multicuts at different thresholds often produces hierarchical clusterings this is by no means guaranteed',\n",
       " 'We have introduced a new method for approximating the closest ultrametric on planar graphs that is applicable to hierarchical image segmentation',\n",
       " 'Our contribution is a dual cutting plane approach that exploits the introduction of novel slack terms that allow for representing a much larger space of solutions with relatively few cutting planes',\n",
       " 'This yields an efficient algorithm that provides rigorous bounds on the quality the resulting solution',\n",
       " 'We empirically observe that our algorithm rapidly produces compelling image segmentations along with lower and upperbounds that are nearly tight on the benchmark BSDS test data set',\n",
       " 'Fitting tree metrics Hierarchical clustering and phylogeny',\n",
       " 'Kappes Thorsten Beier Ullrich Kothe and Fred A',\n",
       " 'Probabilistic image segmentation with closedness constraints',\n",
       " 'Briggman Winfried Denk Natalya Korogod Graham Knott Ullrich Kothe and Fred',\n",
       " 'Globally optimal closedsurface segmentation for connectomics',\n",
       " 'Manjunath Stephen Kirchhoff Engin Turetken Charless Fowlkes and Hanspeter Pfister',\n",
       " 'Segmenting planar superpixel adjacency graphs wrt nonplanar superpixel affinity graphs',\n",
       " 'Contour detection and hierarchical image segmentation',\n",
       " 'Optimal coalition structure generation in cooperative graph games',\n",
       " 'On the computational complexity of ising spin glass models',\n",
       " 'On cuts and matchings in planar graphs',\n",
       " 'Cut glue and cut A fast approximate solver for multicut partitioning',\n",
       " 'On the dimer solution of planar ising models',\n",
       " 'Higherorder correlation clustering for image segmentation',\n",
       " 'Blossom v a new implementation of a minimum cost perfect matching algorithm',\n",
       " 'A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics',\n",
       " 'Learning to detect natural image boundaries using local brightness color and texture cues',\n",
       " 'Parallel multicut segmentation via dual decomposition',\n",
       " 'Fast planar correlation clustering for image segmentation',\n",
       " 'Cell detection and segmentation using correlation clustering',\n",
       " 'Since general users often take a series of pictures on their special moments it would better take into consideration of the whole image stream to produce natural language descriptions',\n",
       " 'While almost all previous studies have dealt with the relation between a single image and a single natural sentence our work extends both input and output dimension to a sequence of images and a sequence of sentences',\n",
       " 'To this end we design a multimodal architecture called coherence recurrent convolutional network CRCN which consists of convolutional neural networks bidirectional recurrent neural networks and an entitybased local coherence model',\n",
       " 'Our approach directly learns from vast usergenerated resource of blog posts as textimage parallel training data',\n",
       " 'We demonstrate that our approach outperforms other stateoftheart candidate methods using both quantitative measures eg',\n",
       " 'BLEU and topK recall and user studies via Amazon Mechanical Turk',\n",
       " 'While most of existing work aims at discovering the relation between a single image and a single natural sentence we extend both input and output dimension to a sequence of images and a sequence of sentences which may be an obvious next step toward joint understanding of the visual content of images and language descriptions albeit underaddressed in current literature',\n",
       " 'Our problem setup is motivated by that general users often take a series of pictures on their memorable moments',\n",
       " 'For example many people who visit New York City NYC would capture their experiences with large image streams and thus it would better take the whole photo stream into consideration for the translation to a natural language description',\n",
       " 'Our objective is given a photo stream to automatically produce a sequence of natural language sentences that best describe the essence of the input image set',\n",
       " 'Since our problem deals with learning the semantic relations between long streams of images and text it is more challenging to obtain appropriate textimage parallel corpus than previous research of single sentence generation',\n",
       " 'Our idea to this issue is to directly leverage online natural blog posts as textimage parallel training data because usually a blog consists of a sequence of informative text and multiple representative images that are carefully selected by authors in a way of storytelling',\n",
       " 'Although we focus on the tourism topics in our experiments our approach is completely unsupervised and thus applicable to any domain that has a large set of blog posts with images',\n",
       " 'We evaluate with quantitative measures eg',\n",
       " 'BLEU and TopK recall and user studies via Amazon Mechanical Turk AMT',\n",
       " 'Due to a recent surge of volume of literature on this subject of generating natural language descriptions for image data here we discuss a representative selection of ideas that are closely related to our work',\n",
       " 'Our approach partly involves the text retrieval because we search for candidate sentences for each image of a query sequence from training database',\n",
       " 'However we then create a final paragraph by considering both compatibilities between individual images and text and the coherence that captures text relatedness at the level of sentencetosentence transitions',\n",
       " 'Unlike videos consecutive images in the streams may show sharp changes of visual content which cause the abrupt discontinuity between consecutive sentences',\n",
       " 'Thus the coherence model is more demanded to make output passages fluent',\n",
       " 'Although our method partly take advantage of such recent progress of multimodal neural networks our major novelty is that we integrate it with the coherence model as a unified endtoend architecture to retrieve fluent sequential multiple sentences',\n",
       " 'In the following we compare more previous work that bears a particular resemblance to ours',\n",
       " 'However the model is applied to a video description task of creating a sentence for a given short video clip and does not address the generation of multiple sequential sentences',\n",
       " 'Hence unlike ours there is no mechanism for the coherence between sentences',\n",
       " 'They propose a latent structural SVM framework to learn the semantic relevance relations from text to image sequences',\n",
       " 'However their model is specialized only for the image sequence retrieval and thus not applicable to the natural sentence generation',\n",
       " 'With both quantitative evaluation and user studies we show that our approach is more successful than other stateoftheart alternatives in verbalizing an image stream',\n",
       " 'The training set size is denoted by L  B',\n",
       " 'We assume that blog authors augment their text with multiple images in a semantically meaningful manner',\n",
       " 'In order to decompose each blog into a sequence of images and associated text we first perform text segmentation and then text summarization',\n",
       " 'The purpose of text segmentation is to divide the input blog text into a set of text segments each of which is associated with a single image',\n",
       " 'Thus the number of segments is identical to the number of images in the blog',\n",
       " 'The objective of text summarization is to reduce each text segment into a single key sentence',\n",
       " 'We first divide the blog passage into text blocks according to paragraphs',\n",
       " 'Simply we assign each text block to the image that has the minimum index distance where each text block and image is counted as a single index distance in the blog',\n",
       " 'We summarize each text segment into a single key sentence',\n",
       " 'Its basic idea is to artificially increase the number of training examples by applying transformations horizontal reflection or adding noise to training images',\n",
       " 'We empirically observe that this idea leads better performance in our problem as well',\n",
       " 'The paragraph vector is a neuralnetwork based unsupervised algorithm that learns fixedlength feature representation from variablelength pieces of passage',\n",
       " 'We use pn to denote the paragraph vector representation for text Tn ',\n",
       " 'We then extract a parsed tree for each Tn to identify coreferent entities and grammatical roles of the words',\n",
       " 'Our approach is one level higher we use sentences from training database to author a sequence of sentences for a novel image stream',\n",
       " 'Although our model can be easily extended to use words or phrases as basic building blocks such granularity makes sequences too long to train the language model which may cause several difficulties for learning the RNN models',\n",
       " 'For example the vanishing gradient effect is a wellknown hardship to backpropagate an error signal through a longrange temporal interval',\n",
       " 'We define the CNN and BRNN model for each position separately and the coherence model for a whole data stream',\n",
       " 'The role of BRNN model is to represent a content flow of text sequences',\n",
       " 'In our problem the BRNN is more suitable than the normal RNN because the BRNN can simultaneously model forward and backward streams which allow us to consider both previous and next sentences for each sentence to make the content of a whole sequence interact with one another',\n",
       " 'The exact form of our BRNN is as follows',\n",
       " 'The BRNN takes a sequence of text vectors pt as input',\n",
       " 'We then compute xft and xbt  which are the activations of input units to forward and backward units',\n",
       " 'Unlike other BRNN models we separate the input activation into forward and backward ones with different sets of parameters Wif and Wib  which empirically leads a better performance',\n",
       " 'Then we create two independent forward and backward hidden units denoted by hft and hbt ',\n",
       " 'The final activation of the BRNN ot can be regarded as a description for the content of the sentence at location t which also implicitly encodes the flow of the sentence and its surrounding context in the sequence',\n",
       " 'The BRNN model can capture the flow of text content but it lacks learning the coherence of passage that reflects distributional syntactic and referential information between discourse entities',\n",
       " 'The entity grid is a table where each row corresponds to a discourse \\x0centity and each column represents a sentence',\n",
       " 'After making the entity grid we enumerate the transitions of the grammatical roles of entities in the whole text',\n",
       " 'Combination of CNN RNN and Coherence Model After the ReLU activation layers of the RNN and the coherence model their output ie ot N q goes through two fully connected FC layers whose role is to decide a proper combination of the BRNN language factors and the coherence factors',\n",
       " 'We use the shared parameters for O and q so that the output mixes well the interaction between the content flows and coherency',\n",
       " 'In our tests joint learning outperforms learning the two terms with separate parameters',\n",
       " 'Empirically it improves generalization performance much over a single FC layer with dropout',\n",
       " 'To train our CRCN model we first define the compatibility score between an image stream and a paragraph sequence',\n",
       " 'On the other hand we define the score by an ordered and paired compatibility between a sentence sequence and an image sequence',\n",
       " 'Second we also add the term that measures the relevance relation of coherency between an image sequence and a text sequence',\n",
       " 'The objective based on the maxmargin structured loss encourages aligned imagesentence sequence pairs to have a higher score by a margin than misaligned pairs',\n",
       " 'Since each contrastive example has a random length and is sampled from the dataset of a wide range of content it is extremely unlikely that the negative examples have the same length and the same content order of sentences Optimization',\n",
       " 'We observe that it is better than a simple Gaussian random initialization although our model is not extremely deep',\n",
       " 'We then generate a set of sentence sequence candidates C by concatenating the sentences associated with the Knearest images at each location t',\n",
       " 'Finally we use our learned CRCN model to compute the compatibility score between the query image stream and each sequence candidate according to which we rank the candidates',\n",
       " 'However one major difficulty of this scenario is that there are exponentially many candidates ie C  K N ',\n",
       " 'To resolve this issue we use an approximate divideandconquer strategy we recursively halve the problem into subproblems until the size of the subproblem is manageable',\n",
       " 'Using the beam search idea we first find the topM best sequence candidates in the subproblem of the lowest level and recursively increase the candidate lengths while the maximum candidate size is limited to M ',\n",
       " 'Though it is an approximate search our experiments assure that it achieves almost optimal solutions with plausible combinatorial search mainly because the local fluency and coherence is undoubtedly necessary for the global one',\n",
       " 'That is in order for a whole sentence sequence to be fluent and coherent its any subparts must be as well',\n",
       " 'We compare the performance of our approach with other stateoftheart candidate methods via quantitative measures and user studies using Amazon Mechanical Turk AMT',\n",
       " 'Please refer to the supplementary material for more results and the details of implementation and experimental setting',\n",
       " 'We collect blog datasets of the two topics NYC and Disneyland',\n",
       " 'Then we manually select the travelogue posts that describe stories and events with multiple images',\n",
       " 'For each test post we use the image sequence as a query Iq and the sequence of summarized sentences as groundtruth TG ',\n",
       " 'Each algorithm retrieves the best sequences from training database for a query image sequence and ideally the retrieved sequences match well with TG ',\n",
       " 'Since the training and test data are disjoint each algorithm can only retrieve similar but not identical sentences at best',\n",
       " 'For quantitative measures we exploit two types of metrics of language similarity ie',\n",
       " 'The topK recall RK is the recall rate of a groundtruth retrieval given top K candidates and the median rank indicates the median ranking value of the first retrieved groundtruth',\n",
       " 'A better performance is indicated by higher BLEU CIDEr METEOR RK scores and lower median rank values',\n",
       " 'For all the baselines we create final sentence sequences by concatenating the sentences generated for each image in the query stream',\n",
       " 'A better performance is indicated by higher BLEU CIDEr METEOR RK scores and lower median rank values',\n",
       " 'We also compare between different variants of our method to validate the contributions of key components of our method',\n",
       " 'The second variant is the BRNNonly method denoted by RCN that excludes the entitybased coherence model from our approach',\n",
       " 'Our complete method is denoted by CRCN and this comparison quantifies the improvement by the coherence model',\n",
       " 'Our approach CRCN and RCN outperform with large margins other stateoftheart baselines which generate passages without consideration of sentencetosentence transitions unlike ours',\n",
       " 'Among mRNNbased models the CNNLSTM significantly outperforms the CNNRNN because the LSTM units help learn models from irregular and lengthy data of natural blogs more robustly',\n",
       " 'It shows that the integration of two key components the BRNN and the coherence model indeed contributes the performance improvement',\n",
       " 'The CRCN is only slightly better than the RCN in language metrics but significantly better in retrieval metrics',\n",
       " 'It means that RCN is fine with retrieving fairly good solutions but not good at ranking the only correct solution high compared to CRCN',\n",
       " 'The small margins in language metrics are also attributed by their inherent limitation for example the BLEU focuses on counting the matches of ngram words and thus is not good at comparing between sentences even worse between paragraphs for fully evaluating their fluency and coherency',\n",
       " 'In each set we show a query image stream and text results created by our method and baselines',\n",
       " 'These qualitative examples demonstrate that our approach is more successful to verbalize image sequences that include a variety of content',\n",
       " 'User Studies via Amazon Mechanical Turk We perform user studies using AMT to observe general users preferences between text sequences by different algorithms',\n",
       " 'We present the percentages of responses that turkers vote for our CRCN over baselines',\n",
       " 'In an AMT test we show a query image stream Iq  and a pair of passages generated by our method CRCN and one baseline in a random order',\n",
       " 'We ask turkers to choose more agreed text sequence with Iq ',\n",
       " 'We design the test as a pairwise comparison instead of a multiplechoice question to make answering and analysis easier',\n",
       " 'We obtain answers from three different turkers for each query',\n",
       " 'We also select GloMatch and RCN as the variants of our method',\n",
       " 'The GloMatch is the worst because it uses too weak image representation ie',\n",
       " 'The coherence becomes more critical as the passage is longer',\n",
       " 'This result assures that as passages are longer the coherence becomes more important and thus CRCNs output is more preferred by turkers',\n",
       " 'We proposed an approach for retrieving sentence sequences for an image stream',\n",
       " 'We developed coherence recurrent convolutional network CRCN which consists of convolutional networks bidirectional recurrent networks and entitybased local coherence model',\n",
       " 'With quantitative evaluation and users studies using AMT on large collections of blog posts we demonstrated that our CRCN approach outperformed other stateoftheart candidate methods',\n",
       " 'Modeling Local Coherence An EntityBased Approach',\n",
       " 'Latent Semantic Analysis for Text Segmentation',\n",
       " 'Longterm Recurrent Convolutional Networks for Visual Recognition and Description',\n",
       " 'Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections',\n",
       " 'Delving Deep into Rectifiers Surpassing HumanLevel Performance on ImageNet Classification',\n",
       " 'Framing Image Description as a Ranking Task Data Models and Evaluation Metrics',\n",
       " 'Deep VisualSemantic Alignments for Generating Image Descriptions',\n",
       " 'Joint Photo Stream and Blog Post Summarization and Exploration',\n",
       " 'Baby Talk Understanding and Generating Image Descriptions',\n",
       " 'TreeTalk Composition and Compression of Trees for Image Descriptions',\n",
       " 'METEOR An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments',\n",
       " 'Distributed Representations of Sentences and Documents',\n",
       " 'The Stanford CoreNLP Natural Language Processing Toolkit',\n",
       " 'Deep Captioning with Multimodal Recurrent Neural Networks mRNN',\n",
       " 'Statistical Language Models based on Neural Networks',\n",
       " 'Translating Video Content to Natural Language Descriptions',\n",
       " 'Grounded Compositional Semantics for Finding and Describing Images with Sentences',\n",
       " 'Multimodal Learning with Deep Boltzmann Machines',\n",
       " 'Show and Tell A Neural Image Caption Generator',\n",
       " 'Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework',\n",
       " 'Unfortunately in practice KwikCluster requires a large number of clustering rounds a potential bottleneck for large graphs',\n",
       " 'We demonstrate experimentally that both algorithms outperform the state of the art both in terms of clustering accuracy and running time',\n",
       " 'Correlation clustering serves as a basic means to achieve this goal given a similarity measure between items the goal is to group similar items together and dissimilar items apart',\n",
       " 'In contrast to other clustering approaches the number of clusters is not determined a priori and good solutions aim to balance the tension between grouping all items together versus isolating them',\n",
       " 'The simplest CC variant can be described on a complete signed graph',\n",
       " 'Our goal is to generate a partition of vertices into disjoint sets that minimizes the number of disagreeing edges this equals the number of  edges cut by the clusters plus the number of   edges inside the clusters',\n",
       " 'This metric is commonly called the number of disagreements',\n",
       " 'Two results of a keyword search might refer to the same item but might look different if they come from different sources',\n",
       " 'By building a similarity \\x0cgraph between entities and then applying CC the hope is to cluster duplicate entities in the same group in the context of keyword search this implies a more meaningful and compact list of results',\n",
       " 'Beyond its theoretical guarantees experimentally KwikCluster performs well when combined with local KwikCluster seems like an inherently sequential algorithm and in most cases of interest it requires many peeling rounds',\n",
       " 'This happens because a small number of vertices are clustered per round',\n",
       " 'This can be a bottleneck for large graphs',\n",
       " 'The algorithm employs a simple step that rejects vertices that are executed in parallel but are conflicting however we see in our experiments this seemingly minor coordination step hinders scaleups in a parallel core setting',\n",
       " 'This algorithm achieves the same approximation as KwikCluster in a logarithmic number of rounds in expectation',\n",
       " 'However it performs significant redundant work per iteration in its effort to detect in parallel which vertices should become cluster centers',\n",
       " 'ClusterWild is a coordinationfree parallel CC algorithm that waives consistency in favor of speed',\n",
       " 'The cost we pay is an arbitrarily small loss in ClusterWilds accuracy',\n",
       " 'Our main theoretical innovation for ClusterWild is analyzing the coordinationfree algorithm as a serial variant of KwikCluster that runs on a noisy graph',\n",
       " 'In our experimental evaluation we demonstrate that both algorithms gracefully scale up to graphs with billions of edges',\n",
       " 'Furthermore we compare against state of the art parallel CC algorithms showing that we consistently outperform these algorithms in terms of both running time and clustering accuracy',\n",
       " 'Notation G denotes a graph with n vertices and m edges',\n",
       " 'We denote by dv the positive degree of a vertex ie the number of vertices connected to v with positive edges denotes the positive maximum degree of G and N v denotes the positive neighborhood of v moreover let Cv  v N v',\n",
       " 'Two Parallel Algorithms for Correlation Clustering The formal definition of correlation clustering is given below',\n",
       " 'KwikCluster is a remarkably simple algorithm that approximately solves the above combinatorial problem and operates as follows',\n",
       " 'Having an order among vertices makes the discussion of parallel algorithms more convenient',\n",
       " 'Now assume that v and u are not friends in G and both v and u become cluster centers',\n",
       " 'Moreover assume that v and u have a common unclustered friend say w should w be clustered with v or u',\n",
       " 'After sampling A each of the P threads picks a vertex with the smallest order in A then checks if that vertex can become a cluster center',\n",
       " 'A thread will check in attemptCluster whether its vertex v has any preceding friends that are cluster centers',\n",
       " 'If there are none it will go ahead and label v as cluster center and proceed with creating a cluster',\n",
       " 'If a preceding friend of v is a cluster center then v is labeled as not being a cluster center',\n",
       " 'If a preceding friend of v call it u has not yet received a label ie u is currently being processed and is not yet labeled as cluster center or not then the thread processing v will wait on u to receive a label',\n",
       " 'After processing all vertices in A all threads are synchronized in bulk the clustered vertices are removed a new active set is sampled and the same process is repeated until everything has been clustered',\n",
       " 'ClusterWild speeds up computation by ignoring the first concurrency rule',\n",
       " 'It uniformly samples unclustered vertices and builds clusters around all of them without respecting the rule that cluster centers cannot be friends in G',\n",
       " 'Each thread picks the first ordered vertex remaining in A and using that vertex as a cluster center it creates a cluster around it',\n",
       " 'It peels away the clustered vertices and repeats the same process on the next remaining vertex in A',\n",
       " 'At the end of processing all vertices in A all threads are synchronized in bulk the clustered vertices are removed a new active set is sampled and the parallel clustering is repeated',\n",
       " 'Interestingly abandoning consistency does not incur much loss in the approximation ratio',\n",
       " 'We show how the error introduced in the accuracy of the solution can be bounded',\n",
       " 'The main intuition of why ClusterWild does not introduce too much error is that the chance of two randomly selected vertices being friends is small hence the concurrency rules are infrequently broken',\n",
       " 'In this section we bound the number of rounds required for each algorithms and establish the theoretical speedup one can obtain with P parallel threads',\n",
       " 'We proceed to present our approximation guarantees',\n",
       " 'The omitted proofs can be found in the Appendix',\n",
       " 'The main idea is to track how fast the maximum degree decreases in the remaining graph at the end of each round',\n",
       " 'The main idea is that the the running time of each super step ie round is determined by the straggling thread ie the one that gets assigned the most amount of work plus the time needed for synchronization at the end of each round',\n",
       " 'We assume that threads operate asynchronously within a round and synchronize at the end of a round',\n",
       " 'A memory cell can be writtenread concurrently by multiple threads',\n",
       " 'The time \\x0cspent per round of the algorithm is proportional to the time of the slowest thread',\n",
       " 'The cost of thread synchronization at the end of each batch takes time OP  where P is the number of threads',\n",
       " 'The total computation cost is proportional to the sum of the time spent for all rounds plus the time spent during the bulk synchronization step',\n",
       " 'ClusterWild as a serial procedure on a noisy graph',\n",
       " 'Analyzing ClusterWild is a bit more involved',\n",
       " 'Our guarantees are based on the fact that ClusterWild can be treated as if one was running a peeling algorithm on a noisy graph',\n",
       " 'Since adjacent active vertices can still become cluster centers in ClusterWild one can view the edges between them as deleted by a somewhat unconventional adversary',\n",
       " 'We analyze this new noisy graph and establish our theoretical result',\n",
       " 'We provide a sketch of the proof and delegate the details to the appendix',\n",
       " 'Since ClusterWild ignores the edges among active vertices we treat these edges as deleted',\n",
       " 'In our main result we quantify the loss of clustering accuracy that is caused by ignoring these edges',\n",
       " 'A bad triangle in G is a set of three vertices such that two pairs are joined with a positive edge and one pair is joined with a negative edge',\n",
       " 'Let Tb denote the set of bad triangles in To quantify the cost of ClusterWild we make the below observation',\n",
       " 'To do that we use the following lemma',\n",
       " 'Let Auv  u or v are activated before any other vertices in Suv ',\n",
       " 'However in our experimental section we test the completely asynchronous algorithms against the BSP algorithms of the previous section and observe that they perform quite similarly both in terms of accuracy of clustering and running times',\n",
       " 'Both problems are hard however the general graph setup seems fundamentally harder',\n",
       " 'This means that a vertex only pushes its cluster ID and status cluster centerclusteredunclustered to its friends versus pulling or asking for its friends cluster status',\n",
       " 'This saves a substantial amount of computational effort',\n",
       " 'In the interest of space we present only representative plots of our results full results are given in our appendix',\n",
       " 'As more threads are added the asychronous variants become faster than their BSP counterparts as there are no synchronization barrriers',\n",
       " 'Synchronization rounds The main overhead of the BSP algorithms lies in the need for synchronization rounds',\n",
       " 'Finally on the smaller graphs we were able to test CDK on CDK returns a worse median objective value than both ClusterWild variants',\n",
       " 'In this paper we have presented two parallel algorithms for correlation clustering with nearly linear speedups and provable approximation ratios',\n",
       " 'In the future we intend to implement our algorithms in the distributed environment where synchronization and communication often account for the highest cost',\n",
       " 'Largescale deduplication with constraints using dedupalog',\n",
       " 'Bounding and comparing methods for correlation clustering beyond ilp',\n",
       " 'Correlation clustering from theory to practice',\n",
       " 'Community mining from signed social networks',\n",
       " 'A correlation clustering approach to link classification in signed networks',\n",
       " 'Aggregating inconsistent information ranking and clustering',\n",
       " 'Optimistic concurrency control for distributed unsupervised learning',\n",
       " 'Greedy sequential maximal independent set and matching are parallel on average',\n",
       " 'Correlation clustering in general weighted graphs',\n",
       " 'Near optimal LP rounding algorithm for correlation clustering on complete and complete kpartite graphs',\n",
       " 'Correlation clustering maximizing agreements via semidefinite programming',\n",
       " 'Correlation clustering with a fixed number of clusters',\n",
       " 'Maximizing quadratic programs extending grothendiecks inequality',\n",
       " 'The WebGraph framework I Compression techniques',\n",
       " 'Layered label propagation A multiresolution coordinatefree ordering for compressing social networks',\n",
       " 'Ubicrawler A scalable fully distributed web crawler',\n",
       " 'Software Practice  Experience \\x0cFaster RCNN Towards RealTime Object Detection Stateoftheart object detection networks depend on region proposal algorithms to hypothesize object locations',\n",
       " 'In this work we introduce a Region Proposal Network RPN that shares fullimage convolutional features with the detection network thus enabling nearly costfree region proposals',\n",
       " 'An RPN is a fullyconvolutional network that simultaneously predicts object bounds and objectness scores at each position',\n",
       " 'RPNs are trained endtoend to generate highquality region proposals which are used by Fast RCNN for detection',\n",
       " 'With a simple alternating optimization RPN and Fast RCNN can be trained to share convolutional features',\n",
       " 'Now proposals are the computational bottleneck in stateoftheart detection systems',\n",
       " 'Region proposal methods typically rely on inexpensive features and economical inference schemes',\n",
       " 'Nevertheless the region proposal step still consumes as much running time as the detection network',\n",
       " 'One may note that fast regionbased CNNs take advantage of GPUs while the region proposal methods used in research are implemented on the CPU making such runtime comparisons inequitable',\n",
       " 'An obvious way to accelerate proposal computation is to reimplement it for the GPU',\n",
       " 'This may be an effective engineering solution but reimplementation ignores the downstream detection network and therefore misses important opportunities for sharing computation',\n",
       " 'This work was done when he was an intern at Microsoft Research \\x0ctection networks computation',\n",
       " 'Our observation is that the convolutional conv feature maps used by regionbased detectors like Fast RCNN can also be used for generating region proposals',\n",
       " 'The fc layer is then turned into a conv layer for detecting multiple classspecific objects',\n",
       " 'We discuss OverFeat and MultiBox in more depth later in context with our method',\n",
       " 'To generate region proposals we slide a small network over the conv feature map output by the last shared conv layer',\n",
       " 'Our method detects objects in a wide range of scales and aspect ratios feature map',\n",
       " 'Note that because the mininetwork operates in a slidingwindow fashion the fullyconnected layers are shared across all spatial locations',\n",
       " 'Each anchor is centered at the sliding window in question and is associated with a scale and aspect ratio',\n",
       " 'An important property of our approach is that it is translation invariant both in terms of the anchors and the functions that compute proposals relative to the anchors',\n",
       " 'If one translates an object in an image the proposal should translate and the same function should be able to predict the proposal in either location',\n",
       " 'A Loss Function for Learning Region Proposals For training RPNs we assign a binary class label of being an object or not to each anchor',\n",
       " 'Note that a single groundtruth box may assign positive labels to multiple anchors',\n",
       " 'For simplicity we implement the cls layer as a twoclass softmax layer',\n",
       " 'Alternatively one may use logistic regression to produce k scores \\x0cHere i is the index of an anchor in a minibatch and pi is the predicted probability of anchor i being an object',\n",
       " 'The classification loss Lcls is log loss over two classes object vs not object',\n",
       " 'The outputs of the cls and reg layers consist of pi  and ti  respectively',\n",
       " 'To account for varying sizes a set of k boundingbox regressors are learned',\n",
       " 'Each regressor is responsible for one scale and one aspect ratio and the k regressors do not share weights',\n",
       " 'As such it is still possible to predict boxes of various sizes even though the features are of a fixed sizescale',\n",
       " 'Each minibatch arises from a single image that contains many positive and negative anchors',\n",
       " 'It is possible to optimize for the loss functions of all anchors but this will bias towards negative samples as they are dominate',\n",
       " 'Our implementation uses Caffe Sharing Convolutional Features for Region Proposal and Object Detection Thus far we have described how to train a network for region proposal generation without considering the regionbased object detection CNN that will utilize these proposals',\n",
       " 'Both RPN and Fast RCNN trained independently will modify their conv layers in different ways',\n",
       " 'We therefore need to develop a technique that allows for sharing conv layers between the two networks rather than learning two separate networks',\n",
       " 'Note that this is not as easy as simply defining a single network that includes both RPN and Fast RCNN and then optimizing it jointly with backpropagation',\n",
       " 'Both cls and reg terms are roughly equally weighted in this way \\x0cnot clear a priori if learning Fast RCNN while simultaneously changing the proposal mechanism will converge',\n",
       " 'In the first step we train the RPN as described above',\n",
       " 'This network is initialized with an ImageNetpretrained model and finetuned endtoend for the region proposal task',\n",
       " 'This detection network is also initialized by the ImageNetpretrained model',\n",
       " 'At this point the two networks do not share conv layers',\n",
       " 'In the third step we use the detector network to initialize RPN training but we fix the shared conv layers and only finetune the layers unique to RPN',\n",
       " 'Now the two networks share conv layers',\n",
       " 'Finally keeping the shared conv layers fixed we finetune the fc layers of the Fast RCNN',\n",
       " 'As such both networks share the same conv layers and form a unified network',\n",
       " 'Even such a large stride provides good results though accuracy may be further improved with a smaller stride',\n",
       " 'We note that our algorithm allows the use of anchor boxes that are larger than the underlying receptive field when predicting large proposals',\n",
       " 'With this design our solution does not need multiscale features or multiscale sliding windows to predict large regions saving considerable running time',\n",
       " 'During training we ignore all crossboundary anchors so they do not contribute to the loss',\n",
       " 'If the boundarycrossing outliers are not ignored in training they introduce large difficult to correct error terms in the objective and training does not converge',\n",
       " 'During testing however we still apply the fullyconvolutional RPN to the entire image',\n",
       " 'This may generate crossboundary proposal boxes which we clip to the image boundary',\n",
       " 'Some RPN proposals highly overlap with each other',\n",
       " 'To reduce redundancy we adopt nonmaximum suppression NMS on the proposal regions based on their cls scores',\n",
       " 'As we will show NMS does not harm the ultimate detection accuracy but substantially reduces the number of proposals',\n",
       " 'After NMS we use the topN ranked proposal regions for detection',\n",
       " 'We primarily evaluate detection mean Average Precision mAP because this is the actual metric for object detection rather than focusing on object proposal proxy metrics',\n",
       " 'These results use the ZF net',\n",
       " 'The detectors are Fast RCNN with ZF but using various proposal methods for training and testing proposals by the fast mode',\n",
       " 'Using RPN yields a much faster detection system than using either SS or EB because of shared conv computations the fewer proposals also reduce the regionwise fc cost',\n",
       " 'Next we consider several ablations of RPN and then show that proposal quality improves when using the very deep network',\n",
       " 'To investigate the behavior of RPNs as a proposal method we conducted several ablation studies',\n",
       " 'First we show the effect of sharing conv layers between the RPN and Fast RCNN detection network',\n",
       " 'We observe that this is because in the third step when the detectortuned features are used to finetune the RPN the proposal quality is improved',\n",
       " 'Next we disentangle the RPNs influence on training the Fast RCNN detection network',\n",
       " 'We fix this detector and evaluate the detection mAP by changing the proposal regions used at testtime',\n",
       " 'In these ablation experiments the RPN does not share features with the detector',\n",
       " 'The loss in mAP is because of the inconsistency between the trainingtesting proposals',\n",
       " 'This result serves as the baseline for the following comparisons',\n",
       " 'Next we separately investigate the roles of RPNs cls and reg outputs by turning off either of them at testtime',\n",
       " 'When the cls layer is removed at testtime thus no NMSranking is used we randomly sample N proposals from the unscored regions',\n",
       " 'This shows that the cls scores account for the accuracy of the highest ranked proposals',\n",
       " 'This suggests that the highquality proposals are mainly due to regressed positions',\n",
       " 'The anchor boxes alone are not sufficient for accurate detection',\n",
       " 'See our released code for the profiling of running time',\n",
       " 'We also evaluate the effects of more powerful networks on the proposal quality of RPN alone',\n",
       " 'This is a promising result because it suggests that the proposal quality of RPNVGG is better than that of RPNZF',\n",
       " 'The following experiments justify this hypothesis',\n",
       " 'As shown above this is because the proposals generated by RPNVGG are more accurate than SS',\n",
       " 'Unlike SS that is predefined the RPN is actively trained and benefits from better networks',\n",
       " 'Next we compute the recall of proposals at different IoU ratios with groundtruth boxes',\n",
       " 'It is more appropriate to use this metric to diagnose the proposal method than to evaluate it',\n",
       " 'We compare with SS and EB and the N proposals are the topN ranked ones based on the confidence generated by these methods',\n",
       " 'As we analyzed before this property is mainly attributed to the cls term of the RPN',\n",
       " 'The recall of SS and EB drops more quickly than RPN when the proposals are fewer',\n",
       " 'OverFeat is a onestage classspecific detection pipeline and ours is a twostage cascade consisting of classagnostic proposals and classspecific detections',\n",
       " 'In OverFeat the regionwise features come from a sliding window of one aspect ratio over a scale pyramid',\n",
       " 'These features are used to simultaneously determine the location and category of objects',\n",
       " 'We believe these features lead to more accurate detections',\n",
       " 'To compare the onestage and twostage systems we emulate the OverFeat system and thus also circumvent other differences of implementation details by onestage Fast RCNN',\n",
       " 'Fast RCNN is trained to predict classspecific scores and regress box locations from these sliding windows',\n",
       " 'This experiment justifies the effectiveness of cascaded region proposals and object detection',\n",
       " 'We also note that the onestage system is slower as it has considerably more proposals to process',\n",
       " 'We have presented Region Proposal Networks RPNs for efficient and accurate region proposal generation',\n",
       " 'By sharing convolutional features with the downstream detection network the region proposal step is nearly costfree']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(new_sent))\n",
    "new_sent[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_sent, columns=[\"Sentences\"])\n",
    "df.to_csv('NIPS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
